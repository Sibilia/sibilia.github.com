<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: cluster | Sibilia octopress blog]]></title>
  <link href="http://Sibilia.github.com/blog/categories/cluster/atom.xml" rel="self"/>
  <link href="http://Sibilia.github.com/"/>
  <updated>2013-07-11T21:26:21+03:00</updated>
  <id>http://Sibilia.github.com/</id>
  <author>
    <name><![CDATA[Ilia Sibiryatkin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Репликация дисков по сети с помощью DRBD]]></title>
    <link href="http://Sibilia.github.com/blog/2013/07/10/rieplikatsiia-diskov-po-sieti-s-pomoshchiu-drbd/"/>
    <updated>2013-07-10T16:13:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/07/10/rieplikatsiia-diskov-po-sieti-s-pomoshchiu-drbd</id>
    <content type="html"><![CDATA[<p>В этой статье я рассмотрю установку и настройку распределённой файловой системы с использованием DRBD. Данное решение может применяться для ряда задач в кластерах высокой доступности (High Availability). Для начала, разъясним понятие DRBD:
<img class="center" src="/images/DRBD.gif" title="DRBD" alt="DRBD">
<blockquote><p>DRBD (от англ. Distributed Replicated Block Device — «Распределённое Копируемое Блочное Устройство») — это блочное устройство, обеспечивающее синхронизацию (RAID1) между локальным блочным устройством и удалённым.</p></blockquote></p>

<!--more-->


<h3>Подготовка</h3>

<p>Эта статья будет продолжением предыдущей <a href="http://sibilia.github.io/blog/2013/06/20/ustanovka-i-nastroika-pacemaker/">статьи</a>, однако это не является полностью обязательным требованием. В частности, перед началом установки из предыдущей статьи нужен раздел "Подготовка". Так же я буду использовать из предыдущей статьи имена нод и их ip адреса.</p>

<p>Так же нам потребуется дополнительно два диска (или раздела), по одному на ноду, на которых мы и будем собирать DRBD. Подготовьте их самостоятельно. У меня в статье они будут фигурировать под именами /dev/sdb.</p>

<h3>Установка</h3>

<p>Если посмотреть на <a href="http://www.drbd.org/">сайте разрабтчиков</a>, то там есть несколько активных версий drbd. Основные стабильные сейчас - 8.3.x и 8.4.x. Ситуация с пакетами rpm немного сложная. Я начну с рецепта сборки из исходников, а потом приведу способ загрузки собранных пакетов на "elrepo" и выложу свою сборку.</p>

<h4>Сборка из исходников</h4>

<p>Итак приступим. Я остановился на версии 8.4.х. Сборку рекомендую проводить на отдельной ноде, чтоб не засорять основные сервера. Подготавливаем инструменты для сборки и окружение:
<code>
yum install gcc rpm-build make flex kernel kernel-headers kernel-devel
mkdir -p ~/rpmbuild/{BUILD,RPMS,SOURCES,SPECS,SRPMS}
</code>
Загружаем исходники и собираем:
<code>
cd /usr/src/
wget http://oss.linbit.com/drbd/8.4/drbd-8.4.3.tar.gz
tar -xzf drbd-8.4.3.tar.gz
cd drbd-8.4.3/
./configure --with-km
make rpm
KDIR=/usr/src/kernels/2.6.32-358.11.1.el6.x86_64/ make km-rpm
</code>
Здесь основной момент в том, что мы собираем не только сам drbd на и его модуль для ядра.
Если всё отлично, то после последней команды мы увидим примерно это:
<code>
...
+ exit 0
You have now:
/root/rpmbuild/RPMS/x86_64/drbd-udev-8.4.3-2.el6.x86_64.rpm
/root/rpmbuild/RPMS/x86_64/drbd-km-2.6.32_358.11.1.el6.x86_64-8.4.3-2.el6.x86_64.rpm
/root/rpmbuild/RPMS/x86_64/drbd-8.4.3-2.el6.x86_64.rpm
/root/rpmbuild/RPMS/x86_64/drbd-xen-8.4.3-2.el6.x86_64.rpm
/root/rpmbuild/RPMS/x86_64/drbd-heartbeat-8.4.3-2.el6.x86_64.rpm
/root/rpmbuild/RPMS/x86_64/drbd-debuginfo-8.4.3-2.el6.x86_64.rpm
/root/rpmbuild/RPMS/x86_64/drbd-km-debuginfo-8.4.3-2.el6.x86_64.rpm
/root/rpmbuild/RPMS/x86_64/drbd-utils-8.4.3-2.el6.x86_64.rpm
/root/rpmbuild/RPMS/x86_64/drbd-pacemaker-8.4.3-2.el6.x86_64.rpm
/root/rpmbuild/RPMS/x86_64/drbd-bash-completion-8.4.3-2.el6.x86_64.rpm
</code>
Теперь можно приступать непосредственно к установке этих пакетов. Пакеты содержащие в имени debuginfo не обязательны к установке.
<code>
yum install /root/rpmbuild/RPMS/x86_64/drbd-*.rpm
</code>
Все собранные пакеты я любезно упаковал в <a href="https://www.dropbox.com/s/s28f85rr950rlpi/drbd-8.4.3.rpm.tar.gz">этот</a> архив.</p>

<h4>Установка из репозитория "elrepo"</h4>

<p>Тут сложного ничего нет, подключаем репозиторий, и ставим.
<code>
rpm -Uvh http://elrepo.org/elrepo-release-6-5.el6.elrepo.noarch.rpm
yum install --enablerepo=elrepo drbd84-utils kmod-drbd84
</code>
Однако, стоит заметить что в этом репозитории отсутствуют некоторые пакеты. В частности полезным является <em>drbd-pacemaker</em>, который устанавливает фирменные ресурс агенты от linbit для pacemaker.</p>

<p>Я использовал способ сборки из исходников. Учитывайте этот момент в дальнейшем, если вы ставили из "elrepo"</p>

<p>Теперь можно приступать к настройке.</p>

<h3>Настройка</h3>

<p>Начнём с глобального конфигурационного файла <em>/etc/drbd.d/global_common.conf</em>. В нём прописываются параметры общие для всех <em>ресурсов</em>. Приведу пример (за подрбнастями как обычно в <em>man drbd.conf</em>):
```
global {
}
common {</p>

<pre><code>net {
    protocol C;

    after-sb-0pri discard-younger-primary;
    after-sb-1pri discard-secondary;
    after-sb-2pri disconnect;
}
</code></pre>

<p>}
```
Здесь параметр <em>protocol C</em> обозначает что операция записи считается завершённой, когда и локальный, и сетевой диски сообщают об успешном завершении записи. А параметры начинающиеся на <em>after-sb</em> определяют действия при Split brain, полезно для автоматического восстановления при мелких неполадках.</p>

<p>Теперь приступим к созданию конфигурационного файла ресурса drbd. В нём прописывается имя ресурса, устройство, и сетевой адрес. Конфигурационные файлы ресурсов хранятся в /etc/drbd.d/<name_res>.res</p>

<p>Вот к примеру мой /etc/drbd.d/r0.res:
```
resource r0 {
  volume 0 {</p>

<pre><code>device    /dev/drbd1;
disk      /dev/sdb;
meta-disk internal;
</code></pre>

<p>  }
  on node1 {</p>

<pre><code>address   10.10.0.210:7789;
</code></pre>

<p>  }
  on node2 {</p>

<pre><code>address   10.10.0.211:7789;
</code></pre>

<p>  }
}
```
Здесь <em>device /dev/drbd1</em> это устрйство drbd которое автоматически создастся. В дальнейшем с ним и будем работаем.</p>

<p>При создании нового ресурса, первым шагом необходимо создать метаданные для диска. Выполняем на node1 и node2:
```</p>

<h1>drbdadm create-md r0</h1>

<p>Writing meta data...
initializing activity log
NOT initializing bitmap
New drbd meta data block successfully created.
success
<code>
Теперь подгружаем модуль для ядра и поднимаем наш ресурс:
</code>
modprobe drbd
drbdadm up r0
<code>
Теперь проверяем состояние диска drbd. Это можно делать двумя способами: *drbd-overview* и *cat /proc/drbd*. Разница лишь в том, что второй способ более информативный.
</code></p>

<h1>cat /proc/drbd</h1>

<p> 1: cs:Connected ro:Secondary/Secondary ds:Inconsistent/Inconsistent C r-----</p>

<pre><code>ns:0 nr:0 dw:0 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:1048508
</code></pre>

<p><code>
*ds:Inconsistent/Inconsistent* - это состояние синхрнизации. На данном этапе это нормальное состояние, просто drbd не разобрался ещё кто главный. Поможем ему, выполняем на любой ноде (к примеру на node1) и смотрим сново состояние:
</code></p>

<h1>drbdadm primary --force r0</h1>

<h1>cat /proc/drbd</h1>

<p> 1: cs:SyncSource ro:Primary/Secondary ds:UpToDate/Inconsistent C r-----</p>

<pre><code>ns:680960 nr:0 dw:0 dr:681624 al:0 bm:41 lo:0 pe:3 ua:0 ap:0 ep:1 wo:f oos:370620
    [===========&gt;........] sync'ed: 64.9% (370620/1048508)K
    finish: 0:00:03 speed: 96,840 (96,840) K/sec
</code></pre>

<h1>cat /proc/drbd</h1>

<p> 1: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----</p>

<pre><code>ns:1048508 nr:0 dw:0 dr:1049172 al:0 bm:64 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
</code></pre>

<p>```
Устройство drbd готово к использованию.</p>

<p>У DRBD есть два режима работы: Active-Pacive и Active-Active.</p>

<h4>Режим Active-Pacive</h4>

<p>Начнём с примера настройки режима Active-Pacive, как более простого. В нём одна нода доступна для чтения и записи, а вторая недоступна вовсе, однако она постоянно синхронизируется на блочном уровне. Так как доступна только одна, то нет необходимости использовать кластерные файловые системы (об этом ниже). Мы будем использовать ext4.</p>

<p>Создаём точку монтирования, файловую систему, монтируем (выполняем на ноде которая в данный момент primary):
<code>
mkdir /mnt/data
mkfs.ext4 /dev/drbd1
mount /dev/drbd1 /mnt/data/
</code>
Приступим к интеграции drbd с нашим кластером pacemaker.</p>

<p>Перед добавлением drbd в конфигурацию pacemaker, останавливаем ресурс drbd:
<code>
umount /mnt/data
drbdadm down r0
</code>
Добавляем ресурс для drbd в pacemaker (не забываем запустить кластер перед этим, если он был остановлен):
```</p>

<h1>crm</h1>

<p>crm(live)# configure
crm(live)configure# edit
<code>
Приведу лишь часть конфига которую добавляем для drbd:
</code>
...
primitive drbd_data ocf:linbit:drbd \</p>

<pre><code>    params drbd_resource="r0"
</code></pre>

<p>primitive fs-data ocf:heartbeat:Filesystem \</p>

<pre><code>    params device="/dev/drbd1" directory="/mnt/data" fstype="ext4" \
    meta target-role="Started"
</code></pre>

<p>ms ms_drbd_data drbd_data \</p>

<pre><code>    meta master-max="1" master-node-max="1" clone-max="2" clone-node-max="1" notify="true"
</code></pre>

<p>location fs-data_on_node1 fs-data 1000: node1
location fs-data_on_node2 fs-data 1000: node2
location ms_drbd_data_on_node1 ms_drbd_data 1000: node1
location ms_drbd_data_on_node2 ms_drbd_data 1000: node2
colocation drbd_fs-data inf: ms_drbd_data:Master fs-data
order fs-data_after_drbd inf: ms_drbd_data:promote fs-data:start
...
```
<em>ms</em> используется для Master/Slave ресурсов. Ещё обязательно прописываем <em>order</em> для очерёдности запуска и <em>colocation</em> для обозначения что файловая система подниматся там где у нас Primary drbd.</p>

<p>Теперь применяем конфигурацию и смотрим состояние кластера:
```
crm(live)configure# commit</p>

<h1>crm_mon -fnr</h1>

<p>Node node1: online</p>

<pre><code>    fs-data (ocf::heartbeat:Filesystem):    Started
    nginx1  (ocf::heartbeat:nginx): Started
    ipaddr_215      (ocf::heartbeat:IPaddr2):       Started
    lsyncd:1        (ocf::heartbeat:lsyncd):        Started
    drbd_data:1     (ocf::linbit:drbd):     Master
</code></pre>

<p>Node node2: online</p>

<pre><code>    nginx2  (ocf::heartbeat:nginx): Started
    lsyncd:0        (ocf::heartbeat:lsyncd):        Started
    ipaddr_216      (ocf::heartbeat:IPaddr2):       Started
    drbd_data:0     (ocf::linbit:drbd):     Started
</code></pre>

<p>Inactive resources:</p>

<p>Migration summary:
* Node node2:
* Node node1:
```
Всё отлично. На этом настройка drbd в режиме Active-Pacive закончена.</p>

<h4>Режим Active-Active</h4>

<p>Тут всё немного усложняется тем, что нам уже нужно использовать клсатерные файловые системы. Я приведу пример настройки GFS2.</p>

<p>Для начала нужно изменить конфигурацию drbd, разрешив режим active-active. Привожу изменённый файл <em>/etc/drbd.d/global_common.conf</em>:
```
global {
}
common {</p>

<pre><code>net {
    protocol C;

    allow-two-primaries;

    after-sb-0pri discard-younger-primary;
    after-sb-1pri discard-secondary;
    after-sb-2pri disconnect;
}
</code></pre>

<p>}
<code>
Устанавливаем необходимые пакеты для gfs2:
</code>
yum install gfs2-utils gfs2-cluster
```
Так же для gfs2 нужен <em>cman</em>, но он у нас установлен ещё в предыдущей статье, как и настроен его конфиг /etc/cluster/cluster.conf.</p>

<p>Создаём файловую систему gfs2 на диске drbd (на той, что primary):
```</p>

<h1>mkfs.gfs2 -p lock_dlm -j 2 -t my_cluster:data /dev/drbd1</h1>

<p>This will destroy any data on /dev/drbd1.
It appears to contain: Linux rev 1.0 ext4 filesystem data (extents) (large files) (huge files)</p>

<p>Are you sure you want to proceed? [y/n] y</p>

<p>Device:                    /dev/drbd1
Blocksize:                 4096
Device Size                1.00 GB (262127 blocks)
Filesystem Size:           1.00 GB (262125 blocks)
Journals:                  2
Resource Groups:           4
Locking Protocol:          "lock_dlm"
Lock Table:                "my_cluster:data"
UUID:                      78a43254-2c97-1116-5328-3a313ff433f3
<code>
По поводу параметров: *-j 2* указывает что создавать надо два журнала, *-t my_cluster:data* указываем имя кластера (он прописан в /etc/cluster/cluster.conf) и имя таблицы блокировок.
Отлично, файловая система готова. Останавливаем drbd и правим конфигурацию кластера (Опять же привожу лишь часть для drbd):
</code>
drbdadm down r0
crm
crm(live)# configure
crm(live)configure# edit
<code>
</code>
...
primitive drbd_data ocf:linbit:drbd \</p>

<pre><code>    params drbd_resource="r0"
</code></pre>

<p>primitive fs-data ocf:heartbeat:Filesystem \</p>

<pre><code>    params device="/dev/drbd1" directory="/mnt/data" fstype="gfs2" \
    meta target-role="Started"
</code></pre>

<p>ms ms_drbd_data drbd_data \</p>

<pre><code>    meta master-max="2" master-node-max="1" clone-max="2" clone-node-max="1" notify="true"
</code></pre>

<p>clone fs-data_clone fs-data \</p>

<pre><code>    params clone-max="2" clone-node-max="1"
</code></pre>

<p>location fs-data_on_node1 fs-data_clone 1000: node1
location fs-data_on_node2 fs-data_clone 1000: node2
location ms_drbd_data_on_node1 ms_drbd_data 1000: node1
location ms_drbd_data_on_node2 ms_drbd_data 1000: node2
colocation drbd_fs-data inf: ms_drbd_data:Master fs-data_clone
order fs-data_after_drbd inf: ms_drbd_data:promote fs-data_clone:start
...
```
Отличия от active-slave в том, что мы указываем <em>master-max="2"</em> и само сабой <em>fstype="gfs2"</em>. Так же я добавил <em>clone</em> для <em>fs-data</em>.</p>

<p>Чтож, посмтрим что у нас вышло:
```</p>

<h1>crm_mon -1</h1>

<p>...
Online: [ node1 node2 ]</p>

<p> ipaddr_215     (ocf::heartbeat:IPaddr2):       Started node1
 ipaddr_216     (ocf::heartbeat:IPaddr2):       Started node2
 nginx1 (ocf::heartbeat:nginx): Started node1
 nginx2 (ocf::heartbeat:nginx): Started node2
 Clone Set: lsyncd-cl [lsyncd]</p>

<pre><code> Started: [ node1 node2 ]
</code></pre>

<p> Master/Slave Set: ms_drbd_data [drbd_data]</p>

<pre><code> Masters: [ node1 node2 ]
</code></pre>

<p> Clone Set: fs-data_clone [fs-data]</p>

<pre><code> Started: [ node1 node2 ]
</code></pre>

<h1>drbd-overview</h1>

<p>  1:r0/0  Connected Primary/Primary UpToDate/UpToDate C r----- /mnt/data gfs2 1.0G 259M 766M 26%
```
Вот в принципе и всё. По поводу решения некоторых проблем с drbd я делал заметку <a href="http://sibilia.github.io/blog/2013/01/29/rieshieniie-niekotorykh-probliem-v-drbd/">тут</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Установка и настройка кластера Pacemaker]]></title>
    <link href="http://Sibilia.github.com/blog/2013/06/20/ustanovka-i-nastroika-pacemaker/"/>
    <updated>2013-06-20T12:41:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/06/20/ustanovka-i-nastroika-pacemaker</id>
    <content type="html"><![CDATA[<p><img class="center" src="/images/pcmk-active-passive.png" title="Pacemaker" alt="Pacemaker">
Для построения кластеров high availability (высокой доступности) я в своей работе использую <a href="http://clusterlabs.org/">Pacemaker</a>. Он себя хорошо зарекомендовал на многих проектах, очень гибкий, динамически развивается. Здесь я приведу краткую инструкцию себе на память по его установке и начальной настройке. Я не претендую на полноту изложения, за этим лучше идти в официальную <a href="http://clusterlabs.org/doc/">документацию</a>, благо она неплохо написана. Используемый дистрибутив CentOS 6. Пример будет описан на двух однотипных серверах.</p>

<!--more-->


<h3>Подготовка</h3>

<p>Описывать установку CentOS я не буду, этого материала хватает. Я рекомендую устанавливать минимальный netinstall, всё что нужно сами доставим.</p>

<p>Вкратце опишу что нам нужно перед тем как приступить непосредственно к установке pacemaker:</p>

<ul>
<li>Установленная CentOS.</li>
<li>Настроенная сеть:  сервера должны быть в одной сети (по крайней мере для этого примера).</li>
<li>hostname прописаны в /etc/hosts (как свой так и соседа). В данном примере будут использоваться имена <strong>node1</strong> и <strong>node2</strong>.</li>
<li>Установлен и настроен SSH. Сгенерированы RSA ключи и разнесены по нодам. Доступ между node1 и node2 по ssh осуществляется без пароля. Есть в принципе способ генерировать ключи непосредственно внутри pacemaker, но этот способ я не рассматриваю, если интересно то вам нужно после установки кластера запустить <em>corosync-keygen</em> и разнести файл /etc/corosync/authkey по нодам.</li>
<li>Установлен и настроен NTP.</li>
</ul>


<p>Устанавливать будем из репозитрия pacemaker - <a href="http://clusterlabs.org/rpm-next/">clusterlabs</a>, там стабильная версия и более свежая по сравнение с репозиторием CentOS. Добавляем репозиторий.
<code>
wget -O /etc/yum.repos.d/pacemaker.repo http://clusterlabs.org/rpm-next/rhel-6/clusterlabs.repo
</code>
Для работы с pacemaker я использую crm shell. К моему сожалению он теперь поставляется отдельно, добавим репозиторий для его установки. Можно его не ставить, тогда можно работать с pcmk, но мне удобен больше crm shell.
<code>
wget -O /etc/yum.repos.d/ha-clustering.repo http://download.opensuse.org/repositories/network:/ha-clustering/CentOS_CentOS-6/network:ha-clustering.repo
</code></p>

<h3>Установка</h3>

<p>Текущая стабильная версия pacemaker использует архитектуру CMAN (вместо openais). Вообще, история развития кластера pacemaker/corosync очень багатая на глобальные изменения, но это отделная тема. Если интересна история развития кластеров то могу порекомендовать вот эту <a href="http://mlug.linux.by/?p=413">презентацию</a> от Владислава Богданова, ему кстати большая благодарность.</p>

<p>Приступим к установке:
```</p>

<h1>yum install pacemaker cman crmsh ccs</h1>

<p><code>
После завершения установки, у меня были следующие версии основных пакетов:
</code></p>

<h1>rpm -qa |grep -e pacemaker -e cman -e corosync -e crmsh</h1>

<p>pacemaker-cli-1.1.9-1512.el6.x86_64
pacemaker-1.1.9-1512.el6.x86_64
corosync-1.4.5-35.1.x86_64
pacemaker-libs-1.1.9-1512.el6.x86_64
pacemaker-cluster-libs-1.1.9-1512.el6.x86_64
cman-3.0.12.1-49.el6.x86_64
corosynclib-1.4.5-35.1.x86_64
crmsh-1.2.5-55.6.x86_64
<code>
Копируем пример конфига:
</code>
cp /etc/corosync/corosync.conf.example.udpu /etc/corosync/corosync.conf
<code>
Я использую для связи между нодами клсатера unicast вместо multicast, на сеть нагрузки меньше, защита лучше. Редактируем конфиг для нашей ситауции. Для тонкой настройки рекомендую почитать *man corosync.conf*. Привожу конфиг /etc/corosync/corosync.conf:
</code></p>

<h1>Please read the corosync.conf.5 manual page</h1>

<p>compatibility: whitetank</p>

<p>totem {</p>

<pre><code>    version: 2
    secauth: off
    threads: 0
    transport: udpu
    interface {
            member {
                    memberaddr: 10.10.0.210
            }
            member {
                    memberaddr: 10.10.0.211
            }
            ringnumber: 0
            bindnetaddr: 10.10.0.0
            mcastaddr: 226.98.1.1
            mcastport: 5500
            ttl: 1
    }
</code></pre>

<p>}</p>

<p>logging {</p>

<pre><code>    fileline: off
    to_stderr: no
    to_logfile: yes
    to_syslog: yes
    logfile: /var/log/cluster/corosync.log
    debug: on
    timestamp: on
    logger_subsys {
            subsys: AMF
            debug: off
    }
</code></pre>

<p>}</p>

<p>amf {</p>

<pre><code>    mode: disabled
</code></pre>

<p>}
<code>
Так же не забываем включить сервис Pacemaker:
</code></p>

<h1>cat &lt;&lt; END >> /etc/corosync/service.d/pcmk</h1>

<p>service {</p>

<pre><code>    name: pacemaker
    ver:  1
</code></pre>

<p>}
END
```
По поводу значения <em>ver:  1</em>, он указывает что сервис pacemaker мы будем запускать сами. Для начала оставим так, в дальнейшем его можно поменять на <em>ver:  0</em> тогда pacemaker будет запускаться сам после запуска кластера.</p>

<p>Для CMAN необходим ещё один конфигурационный файл /etc/cluster/cluster.conf. Он имеет формат xml. Писать ручками его не практично, будем использовать установленную ранее <em>ccs</em>. Для начала пропишем имя кластеру и добавим ноды:
```</p>

<h1>ccs -f /etc/cluster/cluster.conf --createcluster my_cluster</h1>

<h1>ccs -f /etc/cluster/cluster.conf --addnode node1</h1>

<p>Node node1 added.</p>

<h1>ccs -f /etc/cluster/cluster.conf --addnode node2</h1>

<p>Node node2 added.
```
Затем, как написано в документации pacemaker:</p>

<blockquote><p>Далее нам нужно научить CMAN как посылать запросы fencing для pacemaker. Мы прописываем это независимо от того, включен fencing в pacemaker, или нет.
```</p>

<h1>ccs -f /etc/cluster/cluster.conf --addfencedev pcmk agent=fence_pcmk</h1>

<h1>ccs -f /etc/cluster/cluster.conf --addmethod pcmk-redirect node1</h1>

<p>Method pcmk-redirect added to node1.</p>

<h1>ccs -f /etc/cluster/cluster.conf --addmethod pcmk-redirect node2</h1>

<p>Method pcmk-redirect added to node2.</p>

<h1>ccs -f /etc/cluster/cluster.conf --addfenceinst pcmk node1 pcmk-redirect port=node1</h1>

<h1>ccs -f /etc/cluster/cluster.conf --addfenceinst pcmk node2 pcmk-redirect port=node2</h1>

<p><code>
После смотрим что у нас получилось, проверяем синтаксис с помощью *ccs_config_validate* и копируем этот конфиг на вторую ноду.
</code></p>

<h1>cat /etc/cluster/cluster.conf</h1>

<p><cluster config_version="8" name="my_cluster">
  <fence_daemon/>
  <clusternodes></p>

<pre><code>&lt;clusternode name="node1" nodeid="1"&gt;
  &lt;fence&gt;
    &lt;method name="pcmk-redirect"&gt;
      &lt;device name="pcmk" port="node1"/&gt;
    &lt;/method&gt;
  &lt;/fence&gt;
&lt;/clusternode&gt;
&lt;clusternode name="node2" nodeid="2"&gt;
  &lt;fence&gt;
    &lt;method name="pcmk-redirect"&gt;
      &lt;device name="pcmk" port="node2"/&gt;
    &lt;/method&gt;
  &lt;/fence&gt;
&lt;/clusternode&gt;
</code></pre>

<p>  </clusternodes>
  <cman/>
  <fencedevices></p>

<pre><code>&lt;fencedevice agent="fence_pcmk" name="pcmk"/&gt;
</code></pre>

<p>  </fencedevices>
  <rm></p>

<pre><code>&lt;failoverdomains/&gt;
&lt;resources/&gt;
</code></pre>

<p>  </rm>
</cluster></p>

<h1>ccs_config_validate</h1>

<p>Configuration validates</p>

<h1>scp /etc/cluster/cluster.conf node2:/etc/cluster/cluster.conf</h1>

<p>```
Всё отлично. Повторяем все предыдущие шаги на node2. Теперь мы имеем подготовленные node1 и node2.</p></blockquote>

<p>Теперь можно запускать кластер, но перед этим ещё одно замечание.</p>

<p>Первоначально, CMAN была написана для rgmanager и предполагает, что кластер не должен запускаться, пока нода не имеет кворума. Поэтому прежде чем пытаться запустить кластер, мы отключим эту особеннсть. Выполняем на node1 и node2:
```</p>

<h1>echo "CMAN_QUORUM_TIMEOUT=0" >> /etc/sysconfig/cman</h1>

<p><code>
Теперь пробуем запустить кластер. Выполняем на node1 и node2:
</code></p>

<h1>service cman start</h1>

<p>Starting cluster:
   Checking if cluster has been disabled at boot...        [  OK  ]
   Checking Network Manager...                             [  OK  ]
   Global setup...                                         [  OK  ]
   Loading kernel modules...                               [  OK  ]
   Mounting configfs...                                    [  OK  ]
   Starting cman...                                        [  OK  ]
   Waiting for quorum...                                   [  OK  ]
   Starting fenced...                                      [  OK  ]
   Starting dlm_controld...                                [  OK  ]
   Tuning DLM kernel config...                             [  OK  ]
   Starting gfs_controld...                                [  OK  ]
   Unfencing self...                                       [  OK  ]
   Joining fence domain...                                 [  OK  ]
<code>
Проверяем ноды:
</code></p>

<h1>cman_tool nodes</h1>

<p>Node  Sts   Inc   Joined               Name
   1   M     44   2013-06-20 15:20:20  node1
   2   M     40   2013-06-20 15:20:20  node2
<code>
Вроде всё чисто, проверяем в /var/log/cluster/corosync.log. Ищем там любые warning и error. Если и там всё отлично, пробуем стартовать pacemaker. Опять же запускаем на node1 и node2:
</code></p>

<h1>service pacemaker start</h1>

<p>Starting cluster:
   Checking if cluster has been disabled at boot...        [  OK  ]
   Checking Network Manager...                             [  OK  ]
   Global setup...                                         [  OK  ]
   Loading kernel modules...                               [  OK  ]
   Mounting configfs...                                    [  OK  ]
   Starting cman...                                        [  OK  ]
   Waiting for quorum...                                   [  OK  ]
   Starting fenced...                                      [  OK  ]
   Starting dlm_controld...                                [  OK  ]
   Tuning DLM kernel config...                             [  OK  ]
   Starting gfs_controld...                                [  OK  ]
   Unfencing self...                                       [  OK  ]
   Joining fence domain...                                 [  OK  ]
Starting Pacemaker Cluster Manager:                        [  OK  ]
```
Снова проверяем /var/log/cluster/corosync.log. Там будет несколько warning по поводу отсутствующей конфигурации /var/lib/pacemaker/cib/cib.xml, но это нормально, мы её сейчас будем создавать.</p>

<p>Проверяем состяние кластера:
```</p>

<h1>crm_mon -1</h1>

<p>Stack: cman
Current DC: node1 - partition with quorum
Version: 1.1.9-1512.el6-2a917dd
2 Nodes configured, unknown expected votes
0 Resources configured.</p>

<p>Online: [ node1 node2 ]
```
Установка закончена. Теперь можно приступать к конфигурированию кластера.</p>

<h3>Конфигурирование pacemaker</h3>

<p>Для управлением конфигурацией кластера будем использовать, как уже раньше отмечалось, crm shell. За разъяснением как работать с pcmk обращайтесь в официальную документацию.</p>

<p>Для начала посмотрим что имеем:
```</p>

<h1>crm configure show</h1>

<p>node node1
node node2
property $id="cib-bootstrap-options" \</p>

<pre><code>    dc-version="1.1.9-1512.el6-2a917dd" \
    cluster-infrastructure="cman"
</code></pre>

<p><code>
Пропишем основные параметры:
</code>
[root@node2 ~]  # crm
crm(live)# configure
crm(live)configure# cib new new_conf
INFO: new_conf shadow CIB created
crm(new_conf)configure# cib use new_conf
crm(new_conf)configure# property stonith-enabled=false
crm(new_conf)configure# property no-quorum-policy=ignore
crm(new_conf)configure# property pe-error-series-max=128
crm(new_conf)configure# property pe-input-series-max=128
crm(new_conf)configure# property pe-warn-series-max=128
crm(new_conf)configure# property symmetric-cluster=false
crm(new_conf)configure# commit
crm(new_conf)configure# cib use live
crm(live)configure# cib commit new_conf
crm(live)configure# show
node node1
node node2
property $id="cib-bootstrap-options" \</p>

<pre><code>    dc-version="1.1.9-1512.el6-2a917dd" \
    cluster-infrastructure="cman" \
    stonith-enabled="false" \
    no-quorum-policy="ignore" \
    pe-error-series-max="128" \
    pe-input-series-max="128" \
    pe-warn-series-max="128" \
    symmetric-cluster="false"
</code></pre>

<p>```
Так, теперь немного комментариев:</p>

<ul>
<li><p><em>crm(live)configure# cib new new_conf</em> - Создаём новую конфигурацию в CIB, в которой и будем делать все изменения. Не рекомендую работать в основной cib (live). Все изменения в новой конфигурации (new_conf) не применяются на кластер до тех пор пока мы её не перенесём в cib live с помощью <em>cib commit new_conf</em>.</p></li>
<li><p><em>property stonith-enabled=false</em> и <em>property no-quorum-policy=ignore</em> - Выключаем fencing и действия по умолчанию при потере кворума.</p></li>
<li><p><em>property symmetric-cluster=false</em> - Переводим кластер в несимметричный режим. В отличии от симметричного, он не запускает нигде никакие ресурсы, если это явно не разрешено в location (об этом ниже). В принципе, в данном случае это нам не нужно, даже добавляет неудобства. Однако это добавляет удобства в конфигурировании если нод больше двух и много ресурсов, которые привязаны только к определённым из них. Я не настаиваю на этом параметре, необходимость в нём зависит от архитектуры кластера.</p></li>
</ul>


<h3>Ресурсы кластера</h3>

<p>Ресурсами в кластере могут являться программы, скрипты, ip адреса, файловые системы и т.д. Вариаций много. Скрипты для управления ресурсами находятся в /usr/lib/ocf/resource.d/. К примеру вот список доступных ресурс агентов для провайдера Heartbeat:
```</p>

<h1>crm ra list ocf heartbeat</h1>

<p>AoEtarget           AudibleAlarm        CTDB                ClusterMon
Delay               Dummy               EvmsSCC             Evmsd
Filesystem          ICP                 IPaddr              IPaddr2
IPsrcaddr           IPv6addr            LVM                 LinuxSCSI
MailTo              ManageRAID          ManageVE            Pure-FTPd
Raid1               Route               SAPDatabase         SAPInstance
SendArp             ServeRAID           SphinxSearchDaemon  Squid
Stateful            SysInfo             VIPArip             VirtualDomain
WAS                 WAS6                WinPopup            Xen
Xinetd              anything            apache              conntrackd
db2                 drbd                eDir88              ethmonitor
exportfs            fio                 iSCSILogicalUnit    iSCSITarget
ids                 iscsi               jboss               lsyncd
lxc                 mysql               mysql-proxy         nfsserver
nginx               oracle              oralsnr             pgsql
pingd               portblock           postfix             proftpd
rsyncd              scsi2reservation    sfex                symlink
syslog-ng           tomcat              vmware
```
Если нет нужного, можно написать свой. Написаны ресурс агенты на bash.­</p>

<p>Приступим к добавлению ресурсов. Создадим к примеру следующую конфигурацию:</p>

<p>Имеется две ноды, на них установлен и настроен nginx, для синхронизации рабочей директории nginx будет использоваться lsyncd/unison. Балансировка будет присходить по двум плавающим ip адресам.</p>

<p>Для начала устанавливаем всё необходимое:
<code>
yum install nginx lsyncd unison
</code>
Про настройку lsyncd я уже <a href="http://sibilia.github.io/blog/2013/03/26/sinkhronizatsiia-failov-s-pomoshchiu-lsyncd-i-unison/">писал</a>. Просто меняем в <em>source</em> и в команде запуска unison /mnt/lsynced на что-нибудь вроде /usr/share/nginx/html. Это конечно зависит от nginx.</p>

<p>Проверяем lsyncd. Запускаем на node2 и создаём файлик на ней же /usr/share/nginx/html/test. В выводе будет что-то вроде:
<code>
[root@node2 ~]  # lsyncd -nodaemon /etc/lsyncd.conf
16:37:59 Normal: Event Create spawns action "/usr/bin/unison -retry 5 -owner -group -batch /usr/share/nginx/html ssh://node1//usr/share/nginx/html"
Contacting server...
Connected [//node1//usr/share/nginx/html -&gt; //node2//usr/share/nginx/html]
...
Synchronization complete  (1 item transferred, 0 skipped, 0 failures)
16:37:59 Normal: Retrying Create on /usr/share/nginx/html//test = 0
</code>
Всё отлично, прерываем lsyncd по ctrl-c.
Ресурс агент для lsyncd я буду использовать свой. Для этого добавляем его на node1 и node2:
<code>
wget -O /usr/lib/ocf/resource.d/heartbeat/lsyncd https://raw.github.com/Sibilia/scripts/master/lsyncd
</code></p>

<p>Настройку nginx приводить не буду, подключайте фантазию.</p>

<p>Теперь приступим к созданию ресурсов в кластере. Для lsyncd я буду использовать clone ресурс, а для nginx два разных ресурса для каждой ноды просто для примера. Выбор того или иного способа описания ресурсов зависит лишь от ситуации и ваших личных предпочтений.
```</p>

<h1>crm</h1>

<p>crm(live)# configure
crm(live)configure# cib use new_conf
crm(new_conf)configure# edit
<code>
И приводим конфигурацию к следующему виуу:
</code>
node node1
node node2
primitive ipaddr_215 ocf:heartbeat:IPaddr2 \</p>

<pre><code>    params ip="10.10.0.215"
</code></pre>

<p>primitive ipaddr_216 ocf:heartbeat:IPaddr2 \</p>

<pre><code>    params ip="10.10.0.216"
</code></pre>

<p>primitive lsyncd ocf:heartbeat:lsyncd \</p>

<pre><code>    params cmdline_options="-log scarce"
</code></pre>

<p>primitive nginx1 ocf:heartbeat:nginx
primitive nginx2 ocf:heartbeat:nginx
clone lsyncd-cl lsyncd
location ip_215-on-node1 ipaddr_215 1000: node1
location ip_215-on-node2 ipaddr_215 500: node2
location ip_216-on-node1 ipaddr_216 500: node1
location ip_216-on-node2 ipaddr_216 1000: node2
location lsyncd-on-node1 lsyncd-cl 500: node1
location lsyncd-on-node2 lsyncd-cl 500: node2
location nginx1-on-node1 nginx1 inf: node1
location nginx2-on-node2 nginx2 inf: node2
property $id="cib-bootstrap-options" \</p>

<pre><code>    dc-version="1.1.9-1512.el6-2a917dd" \
    cluster-infrastructure="cman" \
    stonith-enabled="false" \
    no-quorum-policy="ignore" \
    pe-error-series-max="128" \
    pe-input-series-max="128" \
    pe-warn-series-max="128" \
    symmetric-cluster="false"
</code></pre>

<p><code>
После выходим из редактора и применяем конфигурацию на кластер:
</code>
crm(new_conf)configure# commit
crm(new_conf)configure# cib use
crm(live)configure# cib commit new_conf
INFO: commited 'new_conf' shadow CIB to the cluster
crm(live)configure# show
<code>
Теперь проверяем как всё запустилось:
</code></p>

<h1>crm_mon -fnr1</h1>

<p>Stack: cman
Current DC: node2 - partition with quorum
Version: 1.1.9-1512.el6-2a917dd
2 Nodes configured, unknown expected votes
6 Resources configured.</p>

<p>Node node1: online</p>

<pre><code>    nginx1  (ocf::heartbeat:nginx): Started 
    ipaddr_215      (ocf::heartbeat:IPaddr2):       Started 
    lsyncd:1        (ocf::heartbeat:lsyncd):        Started 
</code></pre>

<p>Node node2: online</p>

<pre><code>    nginx2  (ocf::heartbeat:nginx): Started 
    lsyncd:0        (ocf::heartbeat:lsyncd):        Started 
    ipaddr_216      (ocf::heartbeat:IPaddr2):       Started 
</code></pre>

<p>Inactive resources:</p>

<p>Migration summary:
* Node node2:
* Node node1:
<code>
Проверим случай выпадения одной ноды, после её вернём обратно:
</code></p>

<h1>crm node standby node1</h1>

<h1>crm_mon -fnr1</h1>

<p>Stack: cman
Current DC: node2 - partition with quorum
Version: 1.1.9-1512.el6-2a917dd
2 Nodes configured, unknown expected votes
6 Resources configured.</p>

<p>Node node1: standby
Node node2: online</p>

<pre><code>    nginx2  (ocf::heartbeat:nginx): Started 
    ipaddr_215      (ocf::heartbeat:IPaddr2):       Started 
    lsyncd:0        (ocf::heartbeat:lsyncd):        Started 
    ipaddr_216      (ocf::heartbeat:IPaddr2):       Started 
</code></pre>

<p>Inactive resources:</p>

<p> nginx1 (ocf::heartbeat:nginx): Stopped
 Clone Set: lsyncd-cl [lsyncd]</p>

<pre><code> Started: [ node2 ]
 Stopped: [ lsyncd:1 ]
</code></pre>

<p>Migration summary:
* Node node2:
* Node node1:</p>

<h1>crm node online node1</h1>

<p>```
Убедились что оба ip адреса переплыли на одну ноду.</p>

<h3>Работа с кластером</h3>

<p>В рабочем процессе, мы можем управлять состояниями нод и состоянием/размещением ресурсов.
К примеру выведем ноду из кластера и вернём обратно:
```</p>

<h1>crm node standby node1</h1>

<h1>crm node online node1</h1>

<p><code>
Можем остановить/переместить ресурс:
</code></p>

<h1>crm resource move ipaddr_216 node1</h1>

<h1>crm resource stop ipaddr_216</h1>

<h1>crm resource start ipaddr_216</h1>

<h1>crm resource unmove ipaddr_216</h1>

<p><code>
Все эти действия сразу выполняются. Бывает ситуация, когда ресурс вылетает, тогда pacemaker определяет по мониторингу что он не запущен и сново запускает, увеличевая при этом счётчик таких ошибок. К примеру убъём процесс nginx:
</code></p>

<h1>ps aux |grep nginx |grep -v grep</h1>

<p>root     10796  0.0  0.4  96024  2040 ?        Ss   18:48   0:00 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf
nginx    10798  0.0  0.5  96376  2708 ?        S    18:48   0:00 nginx: worker process</p>

<h1>kill -KILL 10796 10798</h1>

<h1>crm_mon -fnr1</h1>

<p>...
Node node1: online</p>

<pre><code>    nginx1  (ocf::heartbeat:nginx): Started 
    ipaddr_215      (ocf::heartbeat:IPaddr2):       Started 
    lsyncd:1        (ocf::heartbeat:lsyncd):        Started 
</code></pre>

<p>Node node2: online</p>

<pre><code>    nginx2  (ocf::heartbeat:nginx): Started 
    ipaddr_216      (ocf::heartbeat:IPaddr2):       Started 
    lsyncd:0        (ocf::heartbeat:lsyncd):        Started 
</code></pre>

<p>Inactive resources:</p>

<p>Migration summary:
* Node node2:
* Node node1:
   nginx1: migration-threshold=1000000 fail-count=2 last-failure='Thu Jun 20 19:17:19 2013'</p>

<p>Failed actions:</p>

<pre><code>nginx1_monitor_5000 (node=node1, call=276, rc=7, status=complete): not running
</code></pre>

<p><code>
Появилась запись что были ошибки. Если же ресурс не может запуститься, или слишком часто вылетает, то значение счётчика fail-count выставляется в 1000000 и после этого этот ресурс на этой ноде не будет восстонавливаться. А для сброса счётчика можно сделать:
</code></p>

<h1>crm resource cleanup nginx1</h1>

<p>```</p>

<h3>P.S.</h3>

<p>Это лишь самые базовые возможности Pacemaker. Есть ещё очерёдность запуска (order), совместное размещение (colocation), группы и т.д. Если есть какие замечания, пожелания, неточности в статье, оставляйте в комментариях или дишите на почту.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Синхронизация файлов с помощью Lsyncd и Unison]]></title>
    <link href="http://Sibilia.github.com/blog/2013/03/26/sinkhronizatsiia-failov-s-pomoshchiu-lsyncd-i-unison/"/>
    <updated>2013-03-26T13:52:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/03/26/sinkhronizatsiia-failov-s-pomoshchiu-lsyncd-i-unison</id>
    <content type="html"><![CDATA[<p>Для быстрой синхронизации файлов между двумя серверами, когда изменения могут появиться на любом из них, прекрасно подходит связка из <a href="https://code.google.com/p/lsyncd/">lsyncd</a> и <a href="http://www.cis.upenn.edu/~bcpierce/unison/">unison</a>.</p>

<p>Lsyncd - это демон который слушает дерево каталогов и выполняет синхронизацию при событии (inotify или fsevents) на нём. Синхронизировать он может с помощью rsync или любым другим способом, который можно прописать у него в настройках в виде скрипта на Lua.</p>

<p>Unison позволяет синхронизировать файлы между серверами (или локально два различных каталога). В отличие от rsync он позволяет синхронизировать файлы одновременно в обе стороны. В качестве транспорта при синхронизации может быть использован ssh.
Пример буду приводить для CentOS/RedHat, для .deb систем отличия в мелочах. Начнём с установки:</p>

<!-- more -->


<p><code>
yum install lsyncd unison
</code>
Далее правим конфигурационный файл lsyncd: /etc/lsyncd.conf
<div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>settings <span class="o">{</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    <span class="nv">logfile</span>         <span class="o">=</span> <span class="s2">&quot;/mnt/lsynced.lsync.log&quot;</span>,
</span><span class='line'>    <span class="nv">statusFile</span>      <span class="o">=</span> <span class="s2">&quot;/mnt/lsynced.lsync.status&quot;</span>,
</span><span class='line'>    <span class="nv">statusInterval</span>  <span class="o">=</span> 10,
</span><span class='line'>    <span class="nv">maxDelays</span>       <span class="o">=</span> 3,
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="o">}</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;runUnison <span class="o">=</span> <span class="o">[[</span>/usr/bin/unison -retry 5 -owner -group -batch /mnt/lsynced ssh://node2//mnt/lsynced<span class="o">]]</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;runbash <span class="o">=</span> <span class="o">{</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    <span class="nv">onCreate</span> <span class="o">=</span> runUnison,
</span><span class='line'>    <span class="nv">onDelete</span> <span class="o">=</span> runUnison,
</span><span class='line'>    <span class="nv">onModify</span> <span class="o">=</span> runUnison,
</span><span class='line'>    <span class="nv">onMove</span> <span class="o">=</span> runUnison,
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="o">}</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;sync <span class="o">{</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    runbash,
</span><span class='line'>    <span class="nv">maxProcesses</span> <span class="o">=</span> 1,
</span><span class='line'>    <span class="nv">delay</span> <span class="o">=</span> 3,
</span><span class='line'>    <span class="nb">source</span> <span class="o">=</span> <span class="s2">&quot;/mnt/lsynced&quot;</span>,
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
На втором сервере меняем имя хоста в строке вызова unison. Проверить можно запустив lsyncd в режиме nodaemon (или без этой опции и читать логи):
<code>
lsyncd -nodaemon /etc/lsyncd.conf
</code>
Если всё отлично завершаем и переходим к настройке запуска lsyncd под corosync.
Тут есть два способа: использовать lsb ресурс corosync или написать свой ресурс агент. Я рассмотрю первый вариант. Для второго у меня на github лежит ресурс агент, но я его ещё не до конца протестировал.</p>

<h3>Corosync lsyncd LSB ресурс.</h3>

<p>Нам понадобится скрипт lsyncd daemon: /etc/init.d/lsyncd. Он должен входить в пакет lsyncd начиная с версии 2.1. Учитывая один момент в нём:
<div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">LSYNCD_OPTIONS</span><span class="o">=</span><span class="s2">&quot;-pidfile /var/run/lsyncd.pid /etc/lsyncd.conf&quot;</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;if <span class="o">[</span> -e /etc/sysconfig/lsyncd <span class="o">]</span>; <span class="k">then</span>
</span><span class='line'>  . /etc/sysconfig/lsyncd
</span><span class='line'><span class="k">fi</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
нам обязательно надо прописать опции запуска в файле /etc/sysconfig/lsyncd:
<div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">LSYNCD_OPTIONS</span><span class="o">=</span><span class="s2">&quot;-log scarce /etc/lsyncd.conf&quot;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
Опция "-log scarce" уменьшает уровень логирования.
Теперь осталось добавить lsyncd в конфигурацию corosync:
<code>
crm configure primitive lsyncd-node1 lsb:lsyncd op monitor interval="5" meta target-role="Started"
crm configure location loc-lsyncd-on-node1 lsyncd-node1 inf: node1
</code>
На втором сервере делаем соответствующие поправки.
Вот в принципе и всё. Данное решение синхронизации хорошо себя показало на высоких нагрузках. Если есть вопросы, я постораюсь помочь.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Решение некоторых проблем в DRBD]]></title>
    <link href="http://Sibilia.github.com/blog/2013/01/29/rieshieniie-niekotorykh-probliem-v-drbd/"/>
    <updated>2013-01-29T14:39:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/01/29/rieshieniie-niekotorykh-probliem-v-drbd</id>
    <content type="html"><![CDATA[<p>Рассмотрю кратко решение проблем в DRBD Diskless и Split-brain.</p>

<h3>DRBD Diskless</h3>

<p>При выходе из строя дискового массива и его восстановления  можно получить следующую ситуацию в drbd:</p>

<pre><code># drbd-overview
2:r2 Connected Secondary/Primary Diskless/UpToDate C r----
</code></pre>

<h4>Для решения этой проблемы необходимо сбросить мета данные:</h4>

<p>На активной ноде необходимо отмонтировать раздел. Затем на неактивной ноде отключаем ресурс:</p>

<pre><code># drbdadm down r2
</code></pre>

<p>Создаем заново блок мета-данных:</p>

<pre><code># drbdadm create-md r2
 ...
New drbd meta data block successfully created.
</code></pre>

<!-- more -->


<p>Включаем ресурс обратно (должна начаться синхронизация):</p>

<pre><code># drbdadm up r2
# drbdadm connect r2
# drbd-overview 
2:r2 SyncTarget Secondary/Primary Inconsistent/UpToDate C r---- 
[&gt;....................] sync'ed: 0.1% (31796/31796)M queue_delay: 0.0 ms 
</code></pre>

<p>Для изменения скорости синхронизации можно ввести:</p>

<pre><code># drbdsetup /dev/drbd2 syncer -r 10M
</code></pre>

<p>Для задания в настройках необходимо прописать в /etc/drbd.conf :
```
syncer {</p>

<pre><code>rate 100M;
</code></pre>

<p>}
```</p>

<h3>DRBD Split-brain</h3>

<p>Ещё бывает ситуация когда ноды не синхронизируются со следующими признаками:</p>

<pre><code># drbd-overview
3:just StandAlone Primary/Unknown UpToDate/DUnknown r----- /mnt/Just ext3 5.0G 156M 4.6G 4%
</code></pre>

<p>Причиной этого может стать состояние split-brain. Для решения этой проблемы необходимо:
На secondary:</p>

<pre><code># drbdadm disconnect just
# drbdadm -- --discard-my-data connect just
</code></pre>

<p>На primary:</p>

<pre><code># drbdadm connect just
</code></pre>

<p>После синхронизации (если она необходима) всё должно работать:</p>

<pre><code># drbd-overview
3:just Connected Primary/Secondary UpToDate/UpToDate C r----- /mnt/Just ext3 5.0G 156M 4.6G 4%
</code></pre>
]]></content>
  </entry>
  
</feed>
