<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: linux | Sibilia octopress blog]]></title>
  <link href="http://Sibilia.github.com/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://Sibilia.github.com/"/>
  <updated>2013-06-20T19:31:46+03:00</updated>
  <id>http://Sibilia.github.com/</id>
  <author>
    <name><![CDATA[Ilia Sibiryatkin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Установка и настройка Pacemaker]]></title>
    <link href="http://Sibilia.github.com/blog/2013/06/20/ustanovka-i-nastroika-pacemaker/"/>
    <updated>2013-06-20T12:41:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/06/20/ustanovka-i-nastroika-pacemaker</id>
    <content type="html"><![CDATA[<p>Для построения кластеров high availability (высокой доступности) я в своей работе использую <a href="http://clusterlabs.org/">Pacemaker</a>. Он себя хорошо зарекомендовал на многих проектах, очень гибкий, динамически развивается. Здесь я приведу краткую инструкцию себе на память по его установке и начальной настройке. Я не претендую на полноту изложения, за этим лучше идти в официальную <a href="http://clusterlabs.org/doc/">документацию</a>, благо она неплохо написана. Используемый дистрибутив CentOS 6. Пример будет описан на двух однотипных серверах.</p>

<!--more-->


<h3>Подготовка</h3>

<p>Описывать установку CentOS я не буду, этого материала хватает. Я рекомендую устанавливать минимальный netinstall, всё что нужно сами доставим.</p>

<p>Вкратце опишу что нам нужно перед тем как приступить непосредственно к установке pacemaker:</p>

<ul>
<li>Установленная CentOS.</li>
<li>Настроенная сеть:  сервера должны быть в одной сети (по крайней мере для этого примера).</li>
<li>hostname прописаны в /etc/hosts (как свой так и соседа). В данном примере будут использоваться имена <strong>node1</strong> и <strong>node2</strong>.</li>
<li>Установлен и настроен SSH. Сгенерированы RSA ключи и разнесены по нодам. Доступ между node1 и node2 по ssh осуществляется без пароля. Есть в принципе способ генерировать ключи непосредственно внутри pacemaker, но этот способ я не рассматриваю, если интересно то вам нужно после установки кластера запустить <em>corosync-keygen</em> и разнести файл /etc/corosync/authkey по нодам.</li>
<li>Установлен и настроен NTP.</li>
</ul>


<p>Устанавливать будем из репозитрия pacemaker - <a href="http://clusterlabs.org/rpm-next/">clusterlabs</a>, там стабильная версия и более свежая по сравнение с репозиторием CentOS. Добавляем репозиторий.
<code>
wget -O /etc/yum.repos.d/pacemaker.repo http://clusterlabs.org/rpm-next/rhel-6/clusterlabs.repo
</code>
Для работы с pacemaker я использую crm shell. К моему сожалению он теперь поставляется отдельно, добавим репозиторий для его установки. Можно его не ставить, тогда можно работать с pcmk, но мне удобен больше crm shell.
<code>
wget -O /etc/yum.repos.d/ha-clustering.repo http://download.opensuse.org/repositories/network:/ha-clustering/CentOS_CentOS-6/network:ha-clustering.repo
</code></p>

<h3>Установка</h3>

<p>Текущая стабильная версия pacemaker использует архитектуру CMAN (вместо openais). Вообще, история развития кластера pacemaker/corosync очень багатая на глобальные изменения, но это отделная тема. Если интересна история развития кластеров то могу порекомендовать вот эту <a href="http://mlug.linux.by/?p=413">презентацию</a> от Владислава Богданова, ему кстати большая благодарность.</p>

<p>Приступим к установке:
```</p>

<h1>yum install pacemaker cman crmsh</h1>

<p><code>
После завершения установки, у меня были следующие версии основных пакетов:
</code></p>

<h1>rpm -qa |grep -e pacemaker -e cman -e corosync -e crmsh</h1>

<p>pacemaker-cli-1.1.9-1512.el6.x86_64
pacemaker-1.1.9-1512.el6.x86_64
corosync-1.4.5-35.1.x86_64
pacemaker-libs-1.1.9-1512.el6.x86_64
pacemaker-cluster-libs-1.1.9-1512.el6.x86_64
cman-3.0.12.1-49.el6.x86_64
corosynclib-1.4.5-35.1.x86_64
crmsh-1.2.5-55.6.x86_64
<code>
Копируем пример конфига:
</code>
cp /etc/corosync/corosync.conf.example.udpu /etc/corosync/corosync.conf
<code>
Я использую для связи между нодами клсатера unicast вместо multicast, на сеть нагрузки меньше, защита лучше. Редактируем конфиг для нашей ситауции. Для тонкой настройки рекомендую почитать man corosync.conf. Привожу конфиг /etc/corosync/corosync.conf:
</code></p>

<h1>Please read the corosync.conf.5 manual page</h1>

<p>compatibility: whitetank</p>

<p>totem {</p>

<pre><code>    version: 2
    secauth: off
    threads: 0
    transport: udpu
    interface {
            member {
                    memberaddr: 10.10.0.210
            }
            member {
                    memberaddr: 10.10.0.211
            }
            ringnumber: 0
            bindnetaddr: 10.10.0.0
            mcastaddr: 226.98.1.1
            mcastport: 5500
            ttl: 1
    }
</code></pre>

<p>}</p>

<p>logging {</p>

<pre><code>    fileline: off
    to_stderr: no
    to_logfile: yes
    to_syslog: yes
    logfile: /var/log/cluster/corosync.log
    debug: on
    timestamp: on
    logger_subsys {
            subsys: AMF
            debug: off
    }
</code></pre>

<p>}</p>

<p>amf {</p>

<pre><code>    mode: disabled
</code></pre>

<p>}
<code>
Так же не забываем включить сервис Pacemaker:
</code></p>

<h1>cat &lt;&lt; END >> /etc/corosync/service.d/pcmk</h1>

<p>service {</p>

<pre><code>    name: pacemaker
    ver:  1
</code></pre>

<p>}
END
```
По поводу значения <em>ver:  1</em>, он указывает что сервис pacemaker мы будем запускать сами. Для начала оставим так, в дальнейшем его можно поменять на <em>ver:  0</em> тогда pacemaker будет запускаться сам после запуска кластера.</p>

<p>Для CMAN необходим ещё один конфигурационный файл /etc/cluster/cluster.conf :
```
&lt;?xml version="1.0"?>
<cluster config_version="1" name="my_cluster">
  <logging debug="off"/>
  <clusternodes></p>

<pre><code>&lt;clusternode name="node1" nodeid="1"/&gt;
&lt;clusternode name="node2" nodeid="2"/&gt;
</code></pre>

<p>  </clusternodes>
</cluster>
<code>
После проверяем правильность конфигурационного файла cluster.conf:
</code></p>

<h1>ccs_config_validate</h1>

<p>Configuration validates
<code>
Всё отлично. Повторяем все предыдущие шаги на node2. Теперь мы имеем подготовленные node1 и node2. Пробуем запустить кластер. Выполняем на node1 и node2:
</code></p>

<h1>service cman start</h1>

<p>Starting cluster:
   Checking if cluster has been disabled at boot...        [  OK  ]
   Checking Network Manager...                             [  OK  ]
   Global setup...                                         [  OK  ]
   Loading kernel modules...                               [  OK  ]
   Mounting configfs...                                    [  OK  ]
   Starting cman...                                        [  OK  ]
   Waiting for quorum...                                   [  OK  ]
   Starting fenced...                                      [  OK  ]
   Starting dlm_controld...                                [  OK  ]
   Tuning DLM kernel config...                             [  OK  ]
   Starting gfs_controld...                                [  OK  ]
   Unfencing self...                                       [  OK  ]
   Joining fence domain...                                 [  OK  ]
<code>
Проверяем ноды:
</code></p>

<h1>cman_tool nodes</h1>

<p>Node  Sts   Inc   Joined               Name
   1   M     44   2013-06-20 15:20:20  node1
   2   M     40   2013-06-20 15:20:20  node2
<code>
Вроде всё чисто, проверяем в /var/log/cluster/corosync.log. Ищем там любые warning и error. Если и там всё отлично, пробуем стартовать pacemaker. Опять же запускаем на node1 и node2:
</code></p>

<h1>service pacemaker start</h1>

<p>Starting cluster:
   Checking if cluster has been disabled at boot...        [  OK  ]
   Checking Network Manager...                             [  OK  ]
   Global setup...                                         [  OK  ]
   Loading kernel modules...                               [  OK  ]
   Mounting configfs...                                    [  OK  ]
   Starting cman...                                        [  OK  ]
   Waiting for quorum...                                   [  OK  ]
   Starting fenced...                                      [  OK  ]
   Starting dlm_controld...                                [  OK  ]
   Tuning DLM kernel config...                             [  OK  ]
   Starting gfs_controld...                                [  OK  ]
   Unfencing self...                                       [  OK  ]
   Joining fence domain...                                 [  OK  ]
Starting Pacemaker Cluster Manager:                        [  OK  ]
```
Снова проверяем /var/log/cluster/corosync.log. Там будет несколько warning по поводу отсутствующей конфигурации /var/lib/pacemaker/cib/cib.xml, но это нормально, мы её сейчас будем создавать.</p>

<p>Проверяем состяние кластера:
```</p>

<h1>crm_mon -1</h1>

<p>Last updated: Thu Jun 20 15:30:13 2013
Last change: Thu Jun 20 15:25:06 2013 via crmd on node1
Stack: cman
Current DC: node1 - partition with quorum
Version: 1.1.9-1512.el6-2a917dd
2 Nodes configured, unknown expected votes
0 Resources configured.</p>

<p>Online: [ node1 node2 ]
```
Установка закончена. Теперь можно приступать к конфигурированию кластера.</p>

<h3>Конфигурирование pacemaker</h3>

<p>Для управлением конфигурацией кластера будем использовать, как уже раньше отмечалось, crm shell. За разъяснением как работать с pcmk обращайтесь в официальную документацию.</p>

<p>Для начала посмотрим что имеем:
```</p>

<h1>crm configure show</h1>

<p>node node1
node node2
property $id="cib-bootstrap-options" \</p>

<pre><code>    dc-version="1.1.9-1512.el6-2a917dd" \
    cluster-infrastructure="cman"
</code></pre>

<p><code>
Сначало пропишем основные параметры:
</code>
[root@node2 ~]  # crm
crm(live)# configure
crm(live)configure# cib new new_conf
INFO: new_conf shadow CIB created
crm(new_conf)configure# cib use new_conf
crm(new_conf)configure# property stonith-enabled=false
crm(new_conf)configure# property no-quorum-policy=ignore
crm(new_conf)configure# property pe-error-series-max=128
crm(new_conf)configure# property pe-input-series-max=128
crm(new_conf)configure# property pe-warn-series-max=128
crm(new_conf)configure# property symmetric-cluster=false
crm(new_conf)configure# commit
crm(new_conf)configure# cib use live
crm(live)configure# cib commit new_conf
crm(live)configure# show
node node1
node node2
property $id="cib-bootstrap-options" \</p>

<pre><code>    dc-version="1.1.9-1512.el6-2a917dd" \
    cluster-infrastructure="cman" \
    stonith-enabled="false" \
    no-quorum-policy="ignore" \
    pe-error-series-max="128" \
    pe-input-series-max="128" \
    pe-warn-series-max="128" \
    symmetric-cluster="false"
</code></pre>

<p>```
Так, теперь немного комментариев:
- <em>crm(live)configure# cib new new_conf</em> - Создаём новую конфигурацию в CIB, в которой и будем делать все изменения. Не рекомендую работать в основной cib (live). Все изменения в новой конфигурации (new_conf) не применяются на кластер до тех пор пока мы её не перенесём в cib live с помощью <em>cib commit new_conf</em>.
- <em>property stonith-enabled=false</em> и <em>property no-quorum-policy=ignore</em> - Выключаем fencing и действия по умолчанию при потере кворума.
- <em>property symmetric-cluster=false</em> - Переводим кластер в несимметричный режим. В отличии от симметричного, он не запускает нигде никакие ресурсы, если это явно не разрешено в location (об этом ниже). В принципе, в данном случае это нам не нужно, даже добавляет неудобства. Однако это добавляет удобства в конфигурировании если нод больше двух и много ресурсов, которые привязаны только к определённо из них. Я не настаиваю на этом параметре, необходимость в нём зависит от архитектуры кластера.</p>

<h3>Ресурсы кластера</h3>

<p>Ресурсами в кластере могут являться программы, скрипты, ip адреса, файловые системы и т.д. Вариаций много. Скрипты для управления ресурсами находятся в /usr/lib/ocf/resource.d/. К примеру вот список доступных ресурс агентов для провайдера Heartbeat:
```</p>

<h1>crm ra list ocf heartbeat</h1>

<p>AoEtarget       AudibleAlarm    CTDB                ClusterMon  Delay               Dummy
EvmsSCC         Evmsd           Filesystem          ICP         IPaddr              IPaddr2
IPsrcaddr       IPv6addr        LVM                 LinuxSCSI   MailTo              ManageRAID
ManageVE        Pure-FTPd       Raid1               Route       SAPDatabase         SAPInstance
SendArp         ServeRAID       SphinxSearchDaemon  Squid       Stateful            SysInfo
VIPArip         VirtualDomain   WAS                 WAS6        WinPopup            Xen
Xinetd          anything        apache              conntrackd  db2                 drbd
eDir88          ethmonitor      exportfs            fio         iSCSILogicalUnit    iSCSITarget
ids             iscsi           jboss               lxc         mysql               mysql-proxy
nfsserver       nginx           oracle              oralsnr     pgsql               pingd
portblock       postfix         proftpd             rsyncd      scsi2reservation    sfex
symlink         syslog-ng       tomcat              vmware
```
Если нужен не хватает, можно написать свой. Написаны ресурс агенты на bash.</p>

<p>Приступим к добавлению ресурсов. Создадим к примеру следующую конфигурацию:</p>

<p>Имеется две ноды, на них установлен и настроен nginx, для синхронизации рабочей директории nginx будет использоваться lsyncd/unison. Балансировка будет присходить по двум плавающим ip адресам.</p>

<p>Для начала устанавливаем всё необходимое:
<code>
yum install nginx lsyncd unison
</code>
Про настройку lsyncd я уже <a href="http://sibilia.github.io/blog/2013/03/26/sinkhronizatsiia-failov-s-pomoshchiu-lsyncd-i-unison/">писал</a>. Просто меняем в <em>source</em> и в команде запуска unison /mnt/lsynced на что-нибудь вроде /usr/share/nginx/html. Это конечно зависит от nginx.</p>

<p>Проверяем lsyncd. Запускаем на node2 и создаём файлик на ней же /usr/share/nginx/html/test. В выводе будет что-то вроде:
<code>
[root@node2 ~]  # lsyncd -nodaemon /etc/lsyncd.conf
16:37:59 Normal: Event Create spawns action "/usr/bin/unison -retry 5 -owner -group -batch /usr/share/nginx/html ssh://node1//usr/share/nginx/html"
Contacting server...
Connected [//node1//usr/share/nginx/html -&gt; //node2//usr/share/nginx/html]
...
Synchronization complete  (1 item transferred, 0 skipped, 0 failures)
16:37:59 Normal: Retrying Create on /usr/share/nginx/html//test = 0
</code>
Всё отлично, прерываем lsyncd по ctrl-c.
Ресурс агент для lsyncd я буду использовать свой. Для этого добавляем его на node1 и node2:
<code>
wget -O /usr/lib/ocf/resource.d/heartbeat/lsyncd https://raw.github.com/Sibilia/scripts/master/lsyncd
</code></p>

<p>Настройку nginx приводить не буду, подключайте фантазию.</p>

<p>Теперь приступим к созданию ресурсов в кластере. Для lsyncd я буду использовать clone ресурс, а для nginx два разных ресурса для каждой ноды просто для примера. Выбор того или иного способа описания ресурсов зависит лишь от ситуации и ваших личных предпочтений.
```</p>

<h1>crm</h1>

<p>crm(live)# configure
crm(live)configure# cib use new_conf
crm(new_conf)configure# primitive lsyncd ocf:heartbeat:lsyncd params cmdline_options="-log scarce"
crm(new_conf)configure# location lsyncd-on-node1 lsyncd 500: node1
crm(new_conf)configure# location lsyncd-on-node2 lsyncd 500: node2
crm(new_conf)configure# clone lsyncd-cl lsyncd
INFO: resource references in location:lsyncd-on-node1 updated
INFO: resource references in location:lsyncd-on-node2 updated
crm(new_conf)configure# primitive nginx1 ocf:heartbeat:nginx
crm(new_conf)configure# location nginx1-on-node1 nginx1 inf: node1
crm(new_conf)configure# location nginx2-on-node2 nginx2 inf: node2
crm(new_conf)configure# primitive ipaddr_215 ocf:heartbeat:IPaddr2 params ip=10.10.0.215
crm(new_conf)configure# primitive ipaddr_216 ocf:heartbeat:IPaddr2 params ip=10.10.0.216
crm(new_conf)configure# location ip_215-on-node1 ipaddr_215 1000: node1
crm(new_conf)configure# location ip_215-on-node2 ipaddr_215 500: node2
crm(new_conf)configure# location ip_216-on-node2 ipaddr_216 1000: node2
crm(new_conf)configure# location ip_216-on-node1 ipaddr_216 500: node1
crm(new_conf)configure# commit
crm(new_conf)configure# cib use
crm(live)configure# cib commit new_conf
INFO: commited 'new_conf' shadow CIB to the cluster
crm(live)configure# show
node node1
node node2
primitive ipaddr_215 ocf:heartbeat:IPaddr2 \</p>

<pre><code>    params ip="10.10.0.215"
</code></pre>

<p>primitive ipaddr_216 ocf:heartbeat:IPaddr2 \</p>

<pre><code>    params ip="10.10.0.216"
</code></pre>

<p>primitive lsyncd ocf:heartbeat:lsyncd \</p>

<pre><code>    params cmdline_options="-log scarce"
</code></pre>

<p>primitive nginx1 ocf:heartbeat:nginx
primitive nginx2 ocf:heartbeat:nginx
clone lsyncd-cl lsyncd
location ip_215-on-node1 ipaddr_215 1000: node1
location ip_215-on-node2 ipaddr_215 500: node2
location ip_216-on-node1 ipaddr_216 500: node1
location ip_216-on-node2 ipaddr_216 1000: node2
location lsyncd-on-node1 lsyncd-cl 500: node1
location lsyncd-on-node2 lsyncd-cl 500: node2
location nginx1-on-node1 nginx1 inf: node1
location nginx2-on-node2 nginx2 inf: node2
property $id="cib-bootstrap-options" \</p>

<pre><code>    dc-version="1.1.9-1512.el6-2a917dd" \
    cluster-infrastructure="cman" \
    stonith-enabled="false" \
    no-quorum-policy="ignore" \
    pe-error-series-max="128" \
    pe-input-series-max="128" \
    pe-warn-series-max="128" \
    symmetric-cluster="false"
</code></pre>

<p><code>
Для удбства написания конфигурации, можете набрать в crm(new_conf)configure# *edit*, открется редактор (в моём случае vim), и вставить вывод *crm(live)configure# show* прямо туда. После сохраняем и выходим.
Теперь проверяем как всё запустилось:
</code></p>

<h1>crm_mon -fnr1</h1>

<p>Last updated: Thu Jun 20 18:45:48 2013
Last change: Thu Jun 20 18:34:58 2013 via cibadmin on node1
Stack: cman
Current DC: node2 - partition with quorum
Version: 1.1.9-1512.el6-2a917dd
2 Nodes configured, unknown expected votes
6 Resources configured.</p>

<p>Node node1: online</p>

<pre><code>    nginx1  (ocf::heartbeat:nginx): Started 
    ipaddr_215      (ocf::heartbeat:IPaddr2):       Started 
    lsyncd:1        (ocf::heartbeat:lsyncd):        Started 
</code></pre>

<p>Node node2: online</p>

<pre><code>    nginx2  (ocf::heartbeat:nginx): Started 
    lsyncd:0        (ocf::heartbeat:lsyncd):        Started 
    ipaddr_216      (ocf::heartbeat:IPaddr2):       Started 
</code></pre>

<p>Inactive resources:</p>

<p>Migration summary:
* Node node2:
* Node node1:
<code>
Проверим случай выпадения одной ноды, после её вернём обратно:
</code></p>

<h1>crm node standby node1</h1>

<h1># crm_mon -fnr1</h1>

<p>Last updated: Thu Jun 20 18:47:47 2013
Last change: Thu Jun 20 18:46:51 2013 via crm_attribute on node1
Stack: cman
Current DC: node2 - partition with quorum
Version: 1.1.9-1512.el6-2a917dd
2 Nodes configured, unknown expected votes
6 Resources configured.</p>

<p>Node node1: standby
Node node2: online</p>

<pre><code>    nginx2  (ocf::heartbeat:nginx): Started 
    ipaddr_215      (ocf::heartbeat:IPaddr2):       Started 
    lsyncd:0        (ocf::heartbeat:lsyncd):        Started 
    ipaddr_216      (ocf::heartbeat:IPaddr2):       Started 
</code></pre>

<p>Inactive resources:</p>

<p> nginx1 (ocf::heartbeat:nginx): Stopped
 Clone Set: lsyncd-cl [lsyncd]</p>

<pre><code> Started: [ node2 ]
 Stopped: [ lsyncd:1 ]
</code></pre>

<p>Migration summary:
* Node node2:
* Node node1:</p>

<h1>crm node online node1</h1>

<p>```
Убедились что оба ip адреса переплыли на одну ноду.</p>

<h3>Работа с кластером</h3>

<p>В рабочем процессе, мы можем управлять состояниями нод и состоянием/размещением ресурсов.
К примеру выведем ноду из кластера и вернём обратно:
```</p>

<h1>crm node standby node1</h1>

<h1>crm node online node1</h1>

<p><code>
Можем остановить/переместить ресурс:
</code></p>

<h1>crm resource move ipaddr_216 node1</h1>

<h1>crm resource stop ipaddr_216</h1>

<h1>crm resource start ipaddr_216</h1>

<h1>crm resource unmove ipaddr_216</h1>

<p><code>
Все эти действия сразу выполняются. Бывает ситуация, когда ресурс вылетает, тогда pacemaker определяет по мониторингу что он не запущен и сново запускает, увеличевая при этом счётчик таких ошибок. К примеру убъём процесс nginx:
</code></p>

<h1>ps aux |grep nginx |grep -v grep</h1>

<p>root     10796  0.0  0.4  96024  2040 ?        Ss   18:48   0:00 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf
nginx    10798  0.0  0.5  96376  2708 ?        S    18:48   0:00 nginx: worker process</p>

<h1>kill -KILL 10796 10798</h1>

<h1>crm_mon -fnr1</h1>

<p>Node node1: online</p>

<pre><code>    nginx1  (ocf::heartbeat:nginx): Started 
    ipaddr_215      (ocf::heartbeat:IPaddr2):       Started 
    lsyncd:1        (ocf::heartbeat:lsyncd):        Started 
</code></pre>

<p>Node node2: online</p>

<pre><code>    nginx2  (ocf::heartbeat:nginx): Started 
    ipaddr_216      (ocf::heartbeat:IPaddr2):       Started 
    lsyncd:0        (ocf::heartbeat:lsyncd):        Started 
</code></pre>

<p>Inactive resources:</p>

<p>Migration summary:
* Node node2:
* Node node1:
   nginx1: migration-threshold=1000000 fail-count=2 last-failure='Thu Jun 20 19:17:19 2013'</p>

<p>Failed actions:</p>

<pre><code>nginx1_monitor_5000 (node=node1, call=276, rc=7, status=complete): not running
</code></pre>

<p><code>
Появилась запись что были ошибки. Если же ресурс не может запуститься, или слишком часто вылетает, то значение счётчика fail-count выставляется в 1000000 и после этого этот ресурс на этой ноде не будет востонавливаться. Для сброса счётчика можно сделать:
</code></p>

<h1>crm resource cleanup nginx1</h1>

<p>```</p>

<h3>P.S.</h3>

<p>Это лишь самые базовые возможности Pacemaker. Есть ещё очерёдность запуска (order), совместное размешение (colocation), группы и т.д. Если есть какие замечания, пожелания, неточности в статье, оставляйте в комментариях, или пишите на почту.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SSH трюки]]></title>
    <link href="http://Sibilia.github.com/blog/2013/05/28/ssh-trick/"/>
    <updated>2013-05-28T16:17:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/05/28/ssh-trick</id>
    <content type="html"><![CDATA[<h3>Проксирование соединения</h3>

<p>Допустим у вас есть необходимость подключаться к удалённому серверу host-end, к которому прямого доступа нет, и есть ssh доступ к серверу host-forw, с которого есть доступ к host-end. Тогда для удобства можно настроить проксирование ssh соединения. В результате, вместо последвательного подключения к host-forw -> host-end, можно будет подключаться сразу к host-end и соединение будет проксироваться через host-forw автоматически.
Для этого, нужно дбавить в конфиг ssh (~/.ssh/config):
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ForwardAgent yes
</span><span class='line'>Host host-end&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>HostName host-end
</span><span class='line'>Port 22
</span><span class='line'>User admin
</span><span class='line'>ProxyCommand ssh admin@host-forw nc %h %p
</span><span class='line'>Compression yes
</span><span class='line'>ForwardX11 no
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p></span></code></pre></td></tr></table></div></figure></notextile></div>
Из примера видно, что можно все необходимые параметры прописать, а их кстати немало (man ssh_config).
Теперь для подключения достаточно набрать:
<code>
ssh host-end
</code></p>

<h3>Просмотр сетевого трафика с удалённого сервера</h3>

<p>На удалённом сервере зачастую есть сетевой сниффер tcpdump. Но снимать трафик на нём, затем переносить на свой комп для анализа в wireshark неудобно, намного проще направить поток с tcpdump сразу себе в wireshark:
<code>
ssh root@host-end -i eth0 -w - 'port !22' | wireshark -k -i -
</code></p>

<h3>Копирование файлов с удалённого сервера на другой, через локальный комп</h3>

<p>Необходимо перекинуть файлы с одного сервера на другой, но они друг друга не видят. Тогда можем перекинуть с локального, котой их обоих видит:
<code>
ssh root@host1 "tar -cf - /dir-copy" | ssh root@host2 "tar -xf - /dir-past/"
</code></p>

<h3>Запуск локального скрипта на удалённом сервере</h3>

<p><code>
ssh -T user@host &lt; script.sh
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NTP - синхронизация времени]]></title>
    <link href="http://Sibilia.github.com/blog/2013/04/11/ntp-sinkhronizatsiia-vriemieni/"/>
    <updated>2013-04-11T14:23:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/04/11/ntp-sinkhronizatsiia-vriemieni</id>
    <content type="html"><![CDATA[<p>Настроить синхронизацию времени в CentOS не сложно. Для этого нам понадобится NTP. Устанавливаем его:
<code>
yum install ntp
</code>
Далее разрешаем демону ntpd устанавливать аппаратное время. Для этого в файле /etc/sysconfig/ntpd прописываем строку SYNC_HWCLOCK=yes :
<code>
echo SYNC_HWCLOCK=yes &gt;&gt; /etc/sysconfig/ntpd
</code>
Запускаем демон ntpd и добавляем в автозагрузку:
<code>
/etc/init.d/ntpd start
chkconfig ntpd on
chkconfig --list ntpd
</code>
Через некоторое время можно проверить статус синхронизации:
```</p>

<h1>ntpstat</h1>

<p>synchronised to NTP server (86.57.151.12) at stratum 5
   time correct to within 570 ms
   polling server every 64 s
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Синхронизация файлов с помощью Lsyncd и Unison]]></title>
    <link href="http://Sibilia.github.com/blog/2013/03/26/sinkhronizatsiia-failov-s-pomoshchiu-lsyncd-i-unison/"/>
    <updated>2013-03-26T13:52:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/03/26/sinkhronizatsiia-failov-s-pomoshchiu-lsyncd-i-unison</id>
    <content type="html"><![CDATA[<p>Для быстрой синхронизации файлов между двумя серверами, когда изменения могут появиться на любом из них, прекрасно подходит связка из <a href="https://code.google.com/p/lsyncd/">lsyncd</a> и <a href="http://www.cis.upenn.edu/~bcpierce/unison/">unison</a>.</p>

<p>Lsyncd - это демон который слушает дерево каталогов и выполняет синхронизацию при событии (inotify или fsevents) на нём. Синхронизировать он может с помощью rsync или любым другим способом, который можно прописать у него в настройках в виде скрипта на Lua.</p>

<p>Unison позволяет синхронизировать файлы между серверами (или локально два различных каталога). В отличие от rsync он позволяет синхронизировать файлы одновременно в обе стороны. В качестве транспорта при синхронизации может быть использован ssh.
Пример буду приводить для CentOS/RedHat, для .deb систем отличия в мелочах. Начнём с установки:</p>

<!-- more -->


<p><code>
yum install lsyncd unison
</code>
Далее правим конфигурационный файл lsyncd: /etc/lsyncd.conf
<div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>settings <span class="o">{</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    <span class="nv">logfile</span>         <span class="o">=</span> <span class="s2">&quot;/mnt/lsynced.lsync.log&quot;</span>,
</span><span class='line'>    <span class="nv">statusFile</span>      <span class="o">=</span> <span class="s2">&quot;/mnt/lsynced.lsync.status&quot;</span>,
</span><span class='line'>    <span class="nv">statusInterval</span>  <span class="o">=</span> 10,
</span><span class='line'>    <span class="nv">maxDelays</span>       <span class="o">=</span> 3,
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="o">}</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;runUnison <span class="o">=</span> <span class="o">[[</span>/usr/bin/unison -retry 5 -owner -group -batch /mnt/lsynced ssh://node2//mnt/lsynced<span class="o">]]</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;runbash <span class="o">=</span> <span class="o">{</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    <span class="nv">onCreate</span> <span class="o">=</span> runUnison,
</span><span class='line'>    <span class="nv">onDelete</span> <span class="o">=</span> runUnison,
</span><span class='line'>    <span class="nv">onModify</span> <span class="o">=</span> runUnison,
</span><span class='line'>    <span class="nv">onMove</span> <span class="o">=</span> runUnison,
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="o">}</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;sync <span class="o">{</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    runbash,
</span><span class='line'>    <span class="nv">maxProcesses</span> <span class="o">=</span> 1,
</span><span class='line'>    <span class="nv">delay</span> <span class="o">=</span> 3,
</span><span class='line'>    <span class="nb">source</span> <span class="o">=</span> <span class="s2">&quot;/mnt/lsynced&quot;</span>,
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
На втором сервере меняем имя хоста в строке вызова unison. Проверить можно запустив lsyncd в режиме nodaemon (или без этой опции и читать логи):
<code>
lsyncd -nodaemon /etc/lsyncd.conf
</code>
Если всё отлично завершаем и переходим к настройке запуска lsyncd под corosync.
Тут есть два способа: использовать lsb ресурс corosync или написать свой ресурс агент. Я рассмотрю первый вариант. Для второго у меня на github лежит ресурс агент, но я его ещё не до конца протестировал.</p>

<h3>Corosync lsyncd LSB ресурс.</h3>

<p>Нам понадобится скрипт lsyncd daemon: /etc/init.d/lsyncd. Он должен входить в пакет lsyncd начиная с версии 2.1. Учитывая один момент в нём:
<div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">LSYNCD_OPTIONS</span><span class="o">=</span><span class="s2">&quot;-pidfile /var/run/lsyncd.pid /etc/lsyncd.conf&quot;</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;if <span class="o">[</span> -e /etc/sysconfig/lsyncd <span class="o">]</span>; <span class="k">then</span>
</span><span class='line'>  . /etc/sysconfig/lsyncd
</span><span class='line'><span class="k">fi</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
нам обязательно надо прописать опции запуска в файле /etc/sysconfig/lsyncd:
<div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">LSYNCD_OPTIONS</span><span class="o">=</span><span class="s2">&quot;-log scarce /etc/lsyncd.conf&quot;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
Опция "-log scarce" уменьшает уровень логирования.
Теперь осталось добавить lsyncd в конфигурацию corosync:
<code>
crm configure primitive lsyncd-node1 lsb:lsyncd op monitor interval="5" meta target-role="Started"
crm configure location loc-lsyncd-on-node1 lsyncd-node1 inf: node1
</code>
На втором сервере делаем соответствующие поправки.
Вот в принципе и всё. Данное решение синхронизации хорошо себя показало на высоких нагрузках. Если есть вопросы, я постораюсь помочь.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Установка crm shell в Pacemaker]]></title>
    <link href="http://Sibilia.github.com/blog/2013/03/14/ustanovka-crm-shell-v-pacemaker/"/>
    <updated>2013-03-14T14:36:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/03/14/ustanovka-crm-shell-v-pacemaker</id>
    <content type="html"><![CDATA[<p>В начале марта в проекте Pacemaker произошли некоторые изменения, в частности был вырезан crm shell начиная с версии pacemaker-1.1.7.
<blockquote><p>Since late-April, the crm shell is no longer included in the Pacemaker source tree. This change was made at the author's request as it is now maintained as a separate project.</p><footer><strong>ClusterLabs</strong> <cite><a href='https://github.com/ClusterLabs/pacemaker#important-information-about-the-crm-shell'>Important Information About the Crm Shell</a></cite></footer></blockquote></p>

<!-- more -->


<p>Это довольно печально, так как crm shell очень удобный инструмент для ручного управления кластером. Для его добавления необходимо доустановить пакеты "crmsh" и "pssh". Скачать их можно с репозитория <a href="http://download.opensuse.org/repositories/network:/ha-clustering/">crm shell</a>.
Для CentOS 6 x86-64 достаточно следующее:
<code>
yum install pacemaker corosync
cd /tmp
wget http://download.opensuse.org/repositories/network:/ha-clustering/CentOS_CentOS-6/x86_64/crmsh-1.2.5-55.2.x86_64.rpm
wget http://download.opensuse.org/repositories/network:/ha-clustering/CentOS_CentOS-6/x86_64/pssh-2.3.1-15.1.x86_64.rpm
yum install ./crmsh*.rpm ./pssh*.rpm
</code>
На данный момент пакетов crmsh и pssh отсутствуют в официальных репозитариях RedHat/CentOS и в EPEL. Быть может в скором времени их туда добавят, или clusterlabs уладят совместимость.</p>

<h3>P.S.</h3>

<p>Вот кстати мой скрипт для отчистки failcount в выводе crm_mon -fnr. Это порой необходимо для востановления кластера после сбоя и обнуления счётчиков.
<div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span> (corosync_clear)</span> <a href='/downloads/code/corosync_clear'>download</a></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c">#!/bin/bash</span>
</span><span class='line'><span class="c"># Если значение failcount=1000000 у ресурса, то сбрасывается его состояние (crm resource cleanup &lt;res&gt;).</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Author: Ilia Sibiryatkin &lt;Sibvilian@gmail.com&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nv">NODE</span><span class="o">=</span><span class="sb">`</span>crm_node -l |cut -d<span class="s1">&#39; &#39;</span> -f 2<span class="sb">`</span>
</span><span class='line'>
</span><span class='line'><span class="k">for </span>STR in <span class="k">$(</span>crm_mon -f1 | grep fail-count| awk <span class="s1">&#39;{ print $1 &quot;@&quot; $3 }&#39;</span><span class="k">)</span> ; <span class="k">do</span>
</span><span class='line'><span class="k">	</span><span class="nv">RES</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span> <span class="nv">$STR</span> |awk <span class="s1">&#39;BEGIN {FS=&quot;@&quot;}{print (substr($1,0,length($1)-1))}&#39;</span><span class="k">)</span>
</span><span class='line'>        <span class="nv">FAIL</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span> <span class="nv">$STR</span> |awk <span class="s1">&#39;BEGIN {FS=&quot;@&quot;}{print (substr($2,12,length($2)))}&#39;</span><span class="k">)</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">if</span> <span class="o">[[</span> <span class="nv">$FAIL</span> <span class="o">==</span> 1000000 <span class="o">]]</span>; <span class="k">then</span>
</span><span class='line'><span class="k">                </span><span class="nb">echo</span> <span class="s2">&quot;cleanup $RES&quot;</span>
</span><span class='line'>                crm resource cleanup <span class="nv">$RES</span>
</span><span class='line'>        <span class="k">else</span>
</span><span class='line'><span class="k">                </span><span class="nb">echo</span> <span class="s2">&quot;clear fail-count $RES&quot;</span>
</span><span class='line'>                <span class="k">for </span>i in <span class="nv">$NODE</span> ; <span class="k">do</span>
</span><span class='line'><span class="k">                        </span>crm resource failcount <span class="nv">$RES</span> delete <span class="nv">$i</span>
</span><span class='line'>                <span class="k">done </span>
</span><span class='line'><span class="k">        fi</span>
</span><span class='line'><span class="k">done</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>
]]></content>
  </entry>
  
</feed>
