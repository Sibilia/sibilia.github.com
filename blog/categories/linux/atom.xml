<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: linux | Sibilia octopress blog]]></title>
  <link href="http://Sibilia.github.com/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://Sibilia.github.com/"/>
  <updated>2013-03-28T17:35:47+03:00</updated>
  <id>http://Sibilia.github.com/</id>
  <author>
    <name><![CDATA[Ilia Sibiryatkin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Синхронизация файлов с помощью Lsyncd и Unison]]></title>
    <link href="http://Sibilia.github.com/blog/2013/03/26/sinkhronizatsiia-failov-s-pomoshchiu-lsyncd-i-unison/"/>
    <updated>2013-03-26T13:52:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/03/26/sinkhronizatsiia-failov-s-pomoshchiu-lsyncd-i-unison</id>
    <content type="html"><![CDATA[<p>Для быстрой синхронизации файлов между двумя серверами, когда изменения могут появиться на любом из них, прекрасно подходит связка из <a href="https://code.google.com/p/lsyncd/">lsyncd</a> и <a href="http://www.cis.upenn.edu/~bcpierce/unison/">unison</a>.</p>

<p>Lsyncd - это демон который слушает дерево каталогов и выполняет синхронизацию при событии (inotify или fsevents) на нём. Синхронизировать он может с помощью rsync или любым другим способом, который можно прописать у него в настройках в виде скрипта на Lua.</p>

<p>Unison позволяет синхронизировать файлы между серверами (или локально два различных каталога). В отличие от rsync он позволяет синхронизировать файлы одновременно в обе стороны. В качестве транспорта при синхронизации может быть использован ssh.
Пример буду приводить для CentOS/RedHat, для .deb систем отличия в мелочах. Начнём с установки:</p>

<!-- more -->


<p><code>
yum install lsyncd unison
</code>
Далее правим конфигурационный файл lsyncd: /etc/lsyncd.conf
<div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>settings <span class="o">{</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    <span class="nv">logfile</span>         <span class="o">=</span> <span class="s2">&quot;/mnt/lsynced.lsync.log&quot;</span>,
</span><span class='line'>    <span class="nv">statusFile</span>      <span class="o">=</span> <span class="s2">&quot;/mnt/lsynced.lsync.status&quot;</span>,
</span><span class='line'>    <span class="nv">statusInterval</span>  <span class="o">=</span> 10,
</span><span class='line'>    <span class="nv">maxDelays</span>       <span class="o">=</span> 3,
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="o">}</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;runUnison <span class="o">=</span> <span class="o">[[</span>/usr/bin/unison -retry 5 -owner -group -batch /mnt/lsynced ssh://node2//mnt/lsynced<span class="o">]]</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;runbash <span class="o">=</span> <span class="o">{</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    <span class="nv">onCreate</span> <span class="o">=</span> runUnison,
</span><span class='line'>    <span class="nv">onDelete</span> <span class="o">=</span> runUnison,
</span><span class='line'>    <span class="nv">onModify</span> <span class="o">=</span> runUnison,
</span><span class='line'>    <span class="nv">onMove</span> <span class="o">=</span> runUnison,
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="o">}</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;sync <span class="o">{</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    runbash,
</span><span class='line'>    <span class="nv">maxProcesses</span> <span class="o">=</span> 1,
</span><span class='line'>    <span class="nv">delay</span> <span class="o">=</span> 3,
</span><span class='line'>    <span class="nb">source</span> <span class="o">=</span> <span class="s2">&quot;/mnt/lsynced&quot;</span>,
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
На втором сервере меняем имя хоста в строке вызова unison. Проверить можно запустив lsyncd в режиме nodaemon (или без этой опции и читать логи):
<code>
lsyncd -nodaemon /etc/lsyncd.conf
</code>
Если всё отлично завершаем и переходим к настройке запуска lsyncd под corosync.
Тут есть два способа: использовать lsb ресурс corosync или написать свой ресурс агент. Я рассмотрю первый вариант. Для второго у меня на github лежит ресурс агент, но я его ещё не до конца протестировал.</p>

<h3>Corosync lsyncd LSB ресурс.</h3>

<p>Нам понадобится скрипт lsyncd daemon: /etc/init.d/lsyncd. Он должен входить в пакет lsyncd начиная с версии 2.1. Учитывая один момент в нём:
<div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">LSYNCD_OPTIONS</span><span class="o">=</span><span class="s2">&quot;-pidfile /var/run/lsyncd.pid /etc/lsyncd.conf&quot;</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;if <span class="o">[</span> -e /etc/sysconfig/lsyncd <span class="o">]</span>; <span class="k">then</span>
</span><span class='line'>  . /etc/sysconfig/lsyncd
</span><span class='line'><span class="k">fi</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
нам обязательно надо прописать опции запуска в файле /etc/sysconfig/lsyncd:
<div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">LSYNCD_OPTIONS</span><span class="o">=</span><span class="s2">&quot;-log scarce /etc/lsyncd.conf&quot;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
Опция "-log scarce" уменьшает уровень логирования.
Теперь осталось добавить lsyncd в конфигурацию corosync:
<code>
crm configure primitive lsyncd-node1 lsb:lsyncd op monitor interval="5" meta target-role="Started"
crm configure location loc-lsyncd-on-node1 lsyncd-node1 inf: node1
</code>
На втором сервере делаем соответствующие поправки.
Вот в принципе и всё. Данное решение синхронизации хорошо себя показало на высоких нагрузках. Если есть вопросы, я постораюсь помочь.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Установка crm shell в Pacemaker]]></title>
    <link href="http://Sibilia.github.com/blog/2013/03/14/ustanovka-crm-shell-v-pacemaker/"/>
    <updated>2013-03-14T14:36:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/03/14/ustanovka-crm-shell-v-pacemaker</id>
    <content type="html"><![CDATA[<p>В начале марта в проекте Pacemaker произошли некоторые изменения, в частности был вырезан crm shell начиная с версии pacemaker-1.1.7.
<blockquote><p>Since late-April, the crm shell is no longer included in the Pacemaker source tree. This change was made at the author's request as it is now maintained as a separate project.</p><footer><strong>ClusterLabs</strong> <cite><a href='https://github.com/ClusterLabs/pacemaker#important-information-about-the-crm-shell'>Important Information About the Crm Shell</a></cite></footer></blockquote></p>

<!-- more -->


<p>Это довольно печально, так как crm shell очень удобный инструмент для ручного управления кластером. Для его добавления необходимо доустановить пакеты "crmsh" и "pssh". Скачать их можно с репозитория <a href="http://download.opensuse.org/repositories/network:/ha-clustering/">crm shell</a>.
Для CentOS 6 x86_64 достаточно следующее:
<code>
yum install pacemaker corosync
cd /tmp
wget http://download.opensuse.org/repositories/network:/ha-clustering/CentOS_CentOS-6/x86_64/crmsh-1.2.5-55.2.x86_64.rpm
wget http://download.opensuse.org/repositories/network:/ha-clustering/CentOS_CentOS-6/x86_64/pssh-2.3.1-15.1.x86_64.rpm
yum install ./crmsh*.rpm ./pssh*.rpm
</code>
На данный момент пакетов crmsh и pssh отсутствуют в официальных репозитариях RedHat/CentOS и в EPEL. Быть может в скором времени их туда добавят, или clusterlabs уладят совместимость.</p>

<h3>P.S.</h3>

<p>Вот кстати мой скрипт для отчистки failcount в выводе crm_mon -fnr. Это порой необходимо для востановления кластера после сбоя и обнуления счётчиков.
<div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span> (corosync_clear)</span> <a href='/downloads/code/corosync_clear'>download</a></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c">#!/bin/bash</span>
</span><span class='line'><span class="c"># Если значение failcount=1000000 у ресурса, то сбрасывается его состояние (crm resource cleanup &lt;res&gt;).</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Author: Ilia Sibiryatkin &lt;Sibvilian@gmail.com&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nv">NODE</span><span class="o">=</span><span class="sb">`</span>crm_node -l |cut -d<span class="s1">&#39; &#39;</span> -f 2<span class="sb">`</span>
</span><span class='line'>
</span><span class='line'><span class="k">for </span>STR in <span class="k">$(</span>crm_mon -f1 | grep fail-count| awk <span class="s1">&#39;{ print $1 &quot;@&quot; $3 }&#39;</span><span class="k">)</span> ; <span class="k">do</span>
</span><span class='line'><span class="k">	</span><span class="nv">RES</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span> <span class="nv">$STR</span> |awk <span class="s1">&#39;BEGIN {FS=&quot;@&quot;}{print (substr($1,0,length($1)-1))}&#39;</span><span class="k">)</span>
</span><span class='line'>        <span class="nv">FAIL</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span> <span class="nv">$STR</span> |awk <span class="s1">&#39;BEGIN {FS=&quot;@&quot;}{print (substr($2,12,length($2)))}&#39;</span><span class="k">)</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">if</span> <span class="o">[[</span> <span class="nv">$FAIL</span> <span class="o">==</span> 1000000 <span class="o">]]</span>; <span class="k">then</span>
</span><span class='line'><span class="k">                </span><span class="nb">echo</span> <span class="s2">&quot;cleanup $RES&quot;</span>
</span><span class='line'>                crm resource cleanup <span class="nv">$RES</span>
</span><span class='line'>        <span class="k">else</span>
</span><span class='line'><span class="k">                </span><span class="nb">echo</span> <span class="s2">&quot;clear fail-count $RES&quot;</span>
</span><span class='line'>                <span class="k">for </span>i in <span class="nv">$NODE</span> ; <span class="k">do</span>
</span><span class='line'><span class="k">                        </span>crm resource failcount <span class="nv">$RES</span> delete <span class="nv">$i</span>
</span><span class='line'>                <span class="k">done </span>
</span><span class='line'><span class="k">        fi</span>
</span><span class='line'><span class="k">done</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Перенаправление http]]></title>
    <link href="http://Sibilia.github.com/blog/2013/03/04/pierienapravlieniie-http/"/>
    <updated>2013-03-04T13:53:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/03/04/pierienapravlieniie-http</id>
    <content type="html"><![CDATA[<p>Возникла необходимость по наступлению определённого события временно перенаправлять http соединения на другой порт, где слушает nginx с заглушкой. Это легко можно сделать с помошью парочки правил в iptables. Приведу несколько основных моментов из bash скрипта.
Нам нужны две таблицы в новой цепочке: "nat" для непосредственно перенаправления и "filter" для сброса активных сессий. Фильтровать активные сессии необходимо потому, что в таблицу "nat", цепочки PREROUTING, поподают лишь tcp соединения с состоянием NEW. Активные tcp соединения живут примерно 2-3 минуты. Если это не критично, то можно обойтись без их фильтрации. Для этого просто не добавляем все правила в которых указана таблица "filter".
Для начала создадим новую цепочку. Это удобно тем что её можно очистить полностью, когда необходимо вернуть всё к изначальному состоянию.
<code>
iptables -t nat -N HTTP_REDIR
iptables -t filter -N HTTP_REDIR
</code></p>

<!-- more -->


<p>Теперь создадим два правила в цепочках PREROUTING и INPUT в которых будем направлять tcp в нашу цепочку. Эти правила не будут удаляться.
<code>
iptables -t nat -I PREROUTING -p tcp -j HTTP_REDIR
iptables -t filter -I INPUT -p tcp -j HTTP_REDIR
</code>
Теперь, если нам необходимо включить перенаправление, то добавляем правила в нашу цепочку:
<code>
iptables -t nat -A HTTP_REDIR -p tcp --dport 80 -j REDIRECT --to-port 20302
iptables -t filter -A HTTP_REDIR -p tcp --dport 80 -j REJECT --reject-with tcp-reset
</code>
Если необходимо перенаправлять к примеру ещё и https то можно вместо "--dport 80" указать "-m multiport --dports 80,443"
Чтобы убрать перенаправление, просто чистим нашу цепочку.
<code>
iptables -t nat -F HTTP_REDIR
iptables -t filter -F HTTP_REDIR
</code>
Вот в принципе и всё, за всеми подробностями идём в маны iptables, написано там всё очень подробно.</p>

<h4>P.S.</h4>

<p>Нашел более удобный способ сброса активный tcp соединений - conntrack.
Пакет называется по разному, в Debian вроде просто conntrack, в CentOS/RedHat conntrack-tool.
После установки, у нас появляется возможность удобно посмотреть активные соединения и их статус:
<code>
conntrack -L -p tcp --dport 80
</code>
Так же удобно их разом очистить:
<code>
conntrack -D -p tcp --dport 80
</code>
Фильтровать он по multiport не умеет. Суть заключается в том, что просто чистится в ядре таблица соединений tcp по определённому порту. Входящий пакет после этого со статусом не NEW не находится в таблице и сбрасывается с tcp-reset.
В таком случае нет нужды использовать таблицу filter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Установка и настройка Munin]]></title>
    <link href="http://Sibilia.github.com/blog/2013/02/18/ustanovka-i-nastroika-munin/"/>
    <updated>2013-02-18T15:07:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/02/18/ustanovka-i-nastroika-munin</id>
    <content type="html"><![CDATA[<p><a href="http://munin-monitoring.org/">Munin</a> - Это мощная клиент-серверная система мониторинга параметров серверов. Главный сервер munin запускается по cron и опрашивает munin-node сервера собирая с них данные и рисует красивые и наглядные графики. На munin-node серверах демон при подключении главного сервера запускает скрипты плагинов из /etc/munin/plugins/. Плагинов в стандартной установке большое количество. Можно написать и свои.</p>

<!-- more -->


<h3>Установка</h3>

<p>Приведу пример установки клиента и сервера для CentOS/RHEL. Для Debian/Ubuntu всё почти аналогично.
Для начала установим главный сервер:</p>

<pre><code># yum install munin munin-node
</code></pre>

<p>Для клиентов необходим только munin-node:</p>

<pre><code># yum install munin-node
</code></pre>

<h3>Настройка</h3>

<p>Главный файл настройки сервера Munin /etc/munin/munin.conf. В нём необходимо прописать клиентов:
```</p>

<h1>a simple host tree</h1>

<p>[node1]</p>

<pre><code>address 172.16.0.12
use_node_name yes
</code></pre>

<p>[node2]</p>

<pre><code>address 172.16.0.13
use_node_name yes
</code></pre>

<p>[bckp]</p>

<pre><code>address 127.0.0.1
use_node_name yes
</code></pre>

<p><code>
Для настройки клиентов нам необходимо подправить /etc/munin/munin-node.conf:
</code></p>

<h1>Разрешаем подключаться серверу:</h1>

<p>allow ^127.0.0.1$
allow ^172.16.0.10$ # ip address bckp</p>

<h1>указываем host name и ip</h1>

<p>host_name node1
host 172.16.0.12
```
Теперь включим парочку дополнительных плагинов:</p>

<pre><code>ln -s /usr/share/munin/plugins/acpi /etc/munin/plugins/
ln -s /usr/share/munin/plugins/iostat /etc/munin/plugins/
ln -s /usr/share/munin/plugins/iostat_ios /etc/munin/plugins/
</code></pre>

<p>Как видно из примера плагины подключаются созданием симлинка в /etc/munin/plugins/</p>

<h5>Подключение плагинов для MongoDB</h5>

<p>Плагины для MongoDB не идут в стандартной поставке, но их не сложно установить.</p>

<pre><code># wget https://github.com/erh/mongo-munin/archive/master.zip -O /tmp/mongo-munin.zip
# unzip /tmp/mongo-munin.zip /tmp/
# mkdir -p /usr/local/share/munin/plugins
# cp /tmp/mongo-munin-master/mongo_* /usr/local/share/munin/plugins/

# ln -s /usr/local/share/munin/plugins/mongo_btree /etc/munin/plugins/
# ln -s /usr/local/share/munin/plugins/mongo_conn /etc/munin/plugins/
# ln -s /usr/local/share/munin/plugins/mongo_lock /etc/munin/plugins/
# ln -s /usr/local/share/munin/plugins/mongo_mem /etc/munin/plugins/
# ln -s /usr/local/share/munin/plugins/mongo_ops /etc/munin/plugins/
</code></pre>

<p>Не забываем перезагружать munin после правки конфигов и добавления/удаления плагинов.</p>

<pre><code># /etc/init.d/munin-node restart
</code></pre>

<p>Можно проверить просто запустив один из скриптов. Первоначально у меня они не отрабатывали:</p>

<pre><code># /usr/local/share/munin/plugins/mongo_btree
...
  File "/usr/lib64/python2.6/json/decoder.py", line 319, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib64/python2.6/json/decoder.py", line 338, in raw_decode
    raise ValueError("No JSON object could be decoded")
ValueError: No JSON object could be decoded
</code></pre>

<p>Подравив в скрипте вывод значения я выяснил в чём проблема...</p>

<pre><code># vim /usr/local/share/munin/plugins/mongo_btree
</code></pre>

<p><code>
28     raw = urllib2.urlopen(req).read()
29     print raw
30     return json.loads( raw )["serverStatus"]
</code></p>

<pre><code># /usr/local/share/munin/plugins/mongo_btree
You are trying to access MongoDB on the native driver port. For http diagnostic access, add 1000 to the port number
....
</code></pre>

<p>Сново правим скрипт не забыв удалить добавленную строчку на прошлом этапе.
```</p>

<pre><code>url = "http://%s:%d/_status" % (host, port+1000)
</code></pre>

<p>```
Теперь всё в порядке.</p>

<pre><code># /usr/local/share/munin/plugins/mongo_btree
missRatio.value 0
resets.value 0
hits.value 23325605
misses.value 46
accesses.value 23325651
</code></pre>

<p>Не забываем сделать соответствующие поправки в остальных скриптах mongo_</p>

<h5>Подключение плагинов для MySQL</h5>

<p>В стандартной поставке есть плагины для mysql, если нужно, подключаем и их:</p>

<pre><code># ln -s /usr/share/munin/plugins/mysql_* /etc/munin/plugins
</code></pre>

<p>Для их работы необходимы дополнительные пакеты для perl:</p>

<pre><code># yum install perl-Cache-Cache perl-DBD-MySQL perl-IPC-ShareLite
</code></pre>

<p>Если ещё чего не хватит, то это легко выяснить чтением логов /var/log/munin-node/munin-node.log.
Создаём в mysql пользователя для munin:</p>

<pre><code># mysql -u root -p -e 'create user munin'
</code></pre>

<p>Подправим файл /etc/munin/plugin-conf.d/munin-node:</p>

<pre><code>[mysql*]
    user root
    env.mysqlopts --defaults-file=/etc/mysql/my.cnf
    env.mysqluser munin
</code></pre>

<h5>Добавление в автозагрузку</h5>

<p>Добавить в автозагрузку не сложно. Необходимо выполнить на каждом сервере:</p>

<pre><code># chkconfig --add munin-node
# chkconfig munin-node on
</code></pre>

<p>Проверяем:</p>

<pre><code># chkconfig --list munin-node
munin-node      0:off   1:off   2:on    3:on    4:on    5:on    6:off
</code></pre>

<h5>Использование.</h5>

<p>На главном сервере Munin необходим Web сервер. В файле /etc/munin/munin.conf прописывается куда будут генерироваться итоговые .html
Например для стандартных настроек достаточно просто в браузере перейти на ip адресс главного сервера munin: http://172.16.0.10/munin</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Решение некоторых проблем в DRBD]]></title>
    <link href="http://Sibilia.github.com/blog/2013/01/29/rieshieniie-niekotorykh-probliem-v-drbd/"/>
    <updated>2013-01-29T14:39:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/01/29/rieshieniie-niekotorykh-probliem-v-drbd</id>
    <content type="html"><![CDATA[<p>Рассмотрю кратко решение проблем в DRBD Diskless и Split-brain.</p>

<h3>DRBD Diskless</h3>

<p>При выходе из строя дискового массива и его восстановления  можно получить следующую ситуацию в drbd:</p>

<pre><code># drbd-overview
2:r2 Connected Secondary/Primary Diskless/UpToDate C r----
</code></pre>

<h4>Для решения этой проблемы необходимо сбросить мета данные:</h4>

<p>На активной ноде необходимо отмонтировать раздел. Затем на неактивной ноде отключаем ресурс:</p>

<pre><code># drbdadm down r2
</code></pre>

<p>Создаем заново блок мета-данных:</p>

<pre><code># drbdadm create-md r2
 ...
New drbd meta data block successfully created.
</code></pre>

<!-- more -->


<p>Включаем ресурс обратно (должна начаться синхронизация):</p>

<pre><code># drbdadm up r2
# drbdadm connect r2
# drbd-overview 
2:r2 SyncTarget Secondary/Primary Inconsistent/UpToDate C r---- 
[&gt;....................] sync'ed: 0.1% (31796/31796)M queue_delay: 0.0 ms 
</code></pre>

<p>Для изменения скорости синхронизации можно ввести:</p>

<pre><code># drbdsetup /dev/drbd2 syncer -r 10M
</code></pre>

<p>Для задания в настройках необходимо прописать в /etc/drbd.conf :
```
syncer {</p>

<pre><code>rate 100M;
</code></pre>

<p>}
```</p>

<h3>DRBD Split-brain</h3>

<p>Ещё бывает ситуация когда ноды не синхронизируются со следующими признаками:</p>

<pre><code># drbd-overview
3:just StandAlone Primary/Unknown UpToDate/DUnknown r----- /mnt/Just ext3 5.0G 156M 4.6G 4%
</code></pre>

<p>Причиной этого может стать состояние split-brain. Для решения этой проблемы необходимо:
На secondary:</p>

<pre><code># drbdadm disconnect just
# drbdadm -- --discard-my-data connect just
</code></pre>

<p>На primary:</p>

<pre><code># drbdadm connect just
</code></pre>

<p>После синхронизации (если она необходима) всё должно работать:</p>

<pre><code># drbd-overview
3:just Connected Primary/Secondary UpToDate/UpToDate C r----- /mnt/Just ext3 5.0G 156M 4.6G 4%
</code></pre>
]]></content>
  </entry>
  
</feed>
