<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: cluster | Sibilia octopress blog]]></title>
  <link href="http://Sibilia.github.com/blog/categories/cluster/atom.xml" rel="self"/>
  <link href="http://Sibilia.github.com/"/>
  <updated>2013-06-21T15:41:13+03:00</updated>
  <id>http://Sibilia.github.com/</id>
  <author>
    <name><![CDATA[Ilia Sibiryatkin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Установка и настройка кластера Pacemaker]]></title>
    <link href="http://Sibilia.github.com/blog/2013/06/20/ustanovka-i-nastroika-pacemaker/"/>
    <updated>2013-06-20T12:41:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/06/20/ustanovka-i-nastroika-pacemaker</id>
    <content type="html"><![CDATA[<p>Для построения кластеров high availability (высокой доступности) я в своей работе использую <a href="http://clusterlabs.org/">Pacemaker</a>. Он себя хорошо зарекомендовал на многих проектах, очень гибкий, динамически развивается. Здесь я приведу краткую инструкцию себе на память по его установке и начальной настройке. Я не претендую на полноту изложения, за этим лучше идти в официальную <a href="http://clusterlabs.org/doc/">документацию</a>, благо она неплохо написана. Используемый дистрибутив CentOS 6. Пример будет описан на двух однотипных серверах.</p>

<!--more-->


<h3>Подготовка</h3>

<p>Описывать установку CentOS я не буду, этого материала хватает. Я рекомендую устанавливать минимальный netinstall, всё что нужно сами доставим.</p>

<p>Вкратце опишу что нам нужно перед тем как приступить непосредственно к установке pacemaker:</p>

<ul>
<li>Установленная CentOS.</li>
<li>Настроенная сеть:  сервера должны быть в одной сети (по крайней мере для этого примера).</li>
<li>hostname прописаны в /etc/hosts (как свой так и соседа). В данном примере будут использоваться имена <strong>node1</strong> и <strong>node2</strong>.</li>
<li>Установлен и настроен SSH. Сгенерированы RSA ключи и разнесены по нодам. Доступ между node1 и node2 по ssh осуществляется без пароля. Есть в принципе способ генерировать ключи непосредственно внутри pacemaker, но этот способ я не рассматриваю, если интересно то вам нужно после установки кластера запустить <em>corosync-keygen</em> и разнести файл /etc/corosync/authkey по нодам.</li>
<li>Установлен и настроен NTP.</li>
</ul>


<p>Устанавливать будем из репозитрия pacemaker - <a href="http://clusterlabs.org/rpm-next/">clusterlabs</a>, там стабильная версия и более свежая по сравнение с репозиторием CentOS. Добавляем репозиторий.
<code>
wget -O /etc/yum.repos.d/pacemaker.repo http://clusterlabs.org/rpm-next/rhel-6/clusterlabs.repo
</code>
Для работы с pacemaker я использую crm shell. К моему сожалению он теперь поставляется отдельно, добавим репозиторий для его установки. Можно его не ставить, тогда можно работать с pcmk, но мне удобен больше crm shell.
<code>
wget -O /etc/yum.repos.d/ha-clustering.repo http://download.opensuse.org/repositories/network:/ha-clustering/CentOS_CentOS-6/network:ha-clustering.repo
</code></p>

<h3>Установка</h3>

<p>Текущая стабильная версия pacemaker использует архитектуру CMAN (вместо openais). Вообще, история развития кластера pacemaker/corosync очень багатая на глобальные изменения, но это отделная тема. Если интересна история развития кластеров то могу порекомендовать вот эту <a href="http://mlug.linux.by/?p=413">презентацию</a> от Владислава Богданова, ему кстати большая благодарность.</p>

<p>Приступим к установке:
```</p>

<h1>yum install pacemaker cman crmsh ccs</h1>

<p><code>
После завершения установки, у меня были следующие версии основных пакетов:
</code></p>

<h1>rpm -qa |grep -e pacemaker -e cman -e corosync -e crmsh</h1>

<p>pacemaker-cli-1.1.9-1512.el6.x86_64
pacemaker-1.1.9-1512.el6.x86_64
corosync-1.4.5-35.1.x86_64
pacemaker-libs-1.1.9-1512.el6.x86_64
pacemaker-cluster-libs-1.1.9-1512.el6.x86_64
cman-3.0.12.1-49.el6.x86_64
corosynclib-1.4.5-35.1.x86_64
crmsh-1.2.5-55.6.x86_64
<code>
Копируем пример конфига:
</code>
cp /etc/corosync/corosync.conf.example.udpu /etc/corosync/corosync.conf
<code>
Я использую для связи между нодами клсатера unicast вместо multicast, на сеть нагрузки меньше, защита лучше. Редактируем конфиг для нашей ситауции. Для тонкой настройки рекомендую почитать *man corosync.conf*. Привожу конфиг /etc/corosync/corosync.conf:
</code></p>

<h1>Please read the corosync.conf.5 manual page</h1>

<p>compatibility: whitetank</p>

<p>totem {</p>

<pre><code>    version: 2
    secauth: off
    threads: 0
    transport: udpu
    interface {
            member {
                    memberaddr: 10.10.0.210
            }
            member {
                    memberaddr: 10.10.0.211
            }
            ringnumber: 0
            bindnetaddr: 10.10.0.0
            mcastaddr: 226.98.1.1
            mcastport: 5500
            ttl: 1
    }
</code></pre>

<p>}</p>

<p>logging {</p>

<pre><code>    fileline: off
    to_stderr: no
    to_logfile: yes
    to_syslog: yes
    logfile: /var/log/cluster/corosync.log
    debug: on
    timestamp: on
    logger_subsys {
            subsys: AMF
            debug: off
    }
</code></pre>

<p>}</p>

<p>amf {</p>

<pre><code>    mode: disabled
</code></pre>

<p>}
<code>
Так же не забываем включить сервис Pacemaker:
</code></p>

<h1>cat &lt;&lt; END >> /etc/corosync/service.d/pcmk</h1>

<p>service {</p>

<pre><code>    name: pacemaker
    ver:  1
</code></pre>

<p>}
END
```
По поводу значения <em>ver:  1</em>, он указывает что сервис pacemaker мы будем запускать сами. Для начала оставим так, в дальнейшем его можно поменять на <em>ver:  0</em> тогда pacemaker будет запускаться сам после запуска кластера.</p>

<p>Для CMAN необходим ещё один конфигурационный файл /etc/cluster/cluster.conf. Он имеет формат xml. Писать ручками его не практично, будем использовать установленную ранее <em>ccs</em>. Для начала пропишем имя кластеру и добавим ноды:
```</p>

<h1>ccs -f /etc/cluster/cluster.conf --createcluster my_cluster</h1>

<h1>ccs -f /etc/cluster/cluster.conf --addnode node1</h1>

<p>Node node1 added.</p>

<h1>ccs -f /etc/cluster/cluster.conf --addnode node2</h1>

<p>Node node2 added.
```
Затем, как написано в документации pacemaker:</p>

<blockquote><p>Далее нам нужно научить CMAN как посылать запросы fencing для pacemaker. Мы прописываем это независимо от того, включен fencing в pacemaker, или нет.
```</p>

<h1>ccs -f /etc/cluster/cluster.conf --addfencedev pcmk agent=fence_pcmk</h1>

<h1>ccs -f /etc/cluster/cluster.conf --addmethod pcmk-redirect node1</h1>

<p>Method pcmk-redirect added to node1.</p>

<h1>ccs -f /etc/cluster/cluster.conf --addmethod pcmk-redirect node2</h1>

<p>Method pcmk-redirect added to node2.</p>

<h1>ccs -f /etc/cluster/cluster.conf --addfenceinst pcmk node1 pcmk-redirect port=node1</h1>

<h1>ccs -f /etc/cluster/cluster.conf --addfenceinst pcmk node2 pcmk-redirect port=node2</h1>

<p><code>
После смотрим что у нас получилось, проверяем синтаксис с помощью *ccs_config_validate* и копируем этот конфиг на вторую ноду.
</code></p>

<h1>cat /etc/cluster/cluster.conf</h1>

<p><cluster config_version="8" name="my_cluster">
  <fence_daemon/>
  <clusternodes></p>

<pre><code>&lt;clusternode name="node1" nodeid="1"&gt;
  &lt;fence&gt;
    &lt;method name="pcmk-redirect"&gt;
      &lt;device name="pcmk" port="node1"/&gt;
    &lt;/method&gt;
  &lt;/fence&gt;
&lt;/clusternode&gt;
&lt;clusternode name="node2" nodeid="2"&gt;
  &lt;fence&gt;
    &lt;method name="pcmk-redirect"&gt;
      &lt;device name="pcmk" port="node2"/&gt;
    &lt;/method&gt;
  &lt;/fence&gt;
&lt;/clusternode&gt;
</code></pre>

<p>  </clusternodes>
  <cman/>
  <fencedevices></p>

<pre><code>&lt;fencedevice agent="fence_pcmk" name="pcmk"/&gt;
</code></pre>

<p>  </fencedevices>
  <rm></p>

<pre><code>&lt;failoverdomains/&gt;
&lt;resources/&gt;
</code></pre>

<p>  </rm>
</cluster></p>

<h1>ccs_config_validate</h1>

<p>Configuration validates</p>

<h1>scp /etc/cluster/cluster.conf node2:/etc/cluster/cluster.conf</h1>

<p>```
Всё отлично. Повторяем все предыдущие шаги на node2. Теперь мы имеем подготовленные node1 и node2.</p></blockquote>

<p>Теперь можно запускать кластер, но перед этим ещё одно замечание.</p>

<p>Первоначально, CMAN была написана для rgmanager и предполагает, что кластер не должен запускаться, пока нода не имеет кворума. Поэтому прежде чем пытаться запустить кластер, мы отключим эту особеннсть. Выполняем на node1 и node2:
```</p>

<h1>echo "CMAN_QUORUM_TIMEOUT=0" >> /etc/sysconfig/cman</h1>

<p><code>
Теперь пробуем запустить кластер. Выполняем на node1 и node2:
</code></p>

<h1>service cman start</h1>

<p>Starting cluster:
   Checking if cluster has been disabled at boot...        [  OK  ]
   Checking Network Manager...                             [  OK  ]
   Global setup...                                         [  OK  ]
   Loading kernel modules...                               [  OK  ]
   Mounting configfs...                                    [  OK  ]
   Starting cman...                                        [  OK  ]
   Waiting for quorum...                                   [  OK  ]
   Starting fenced...                                      [  OK  ]
   Starting dlm_controld...                                [  OK  ]
   Tuning DLM kernel config...                             [  OK  ]
   Starting gfs_controld...                                [  OK  ]
   Unfencing self...                                       [  OK  ]
   Joining fence domain...                                 [  OK  ]
<code>
Проверяем ноды:
</code></p>

<h1>cman_tool nodes</h1>

<p>Node  Sts   Inc   Joined               Name
   1   M     44   2013-06-20 15:20:20  node1
   2   M     40   2013-06-20 15:20:20  node2
<code>
Вроде всё чисто, проверяем в /var/log/cluster/corosync.log. Ищем там любые warning и error. Если и там всё отлично, пробуем стартовать pacemaker. Опять же запускаем на node1 и node2:
</code></p>

<h1>service pacemaker start</h1>

<p>Starting cluster:
   Checking if cluster has been disabled at boot...        [  OK  ]
   Checking Network Manager...                             [  OK  ]
   Global setup...                                         [  OK  ]
   Loading kernel modules...                               [  OK  ]
   Mounting configfs...                                    [  OK  ]
   Starting cman...                                        [  OK  ]
   Waiting for quorum...                                   [  OK  ]
   Starting fenced...                                      [  OK  ]
   Starting dlm_controld...                                [  OK  ]
   Tuning DLM kernel config...                             [  OK  ]
   Starting gfs_controld...                                [  OK  ]
   Unfencing self...                                       [  OK  ]
   Joining fence domain...                                 [  OK  ]
Starting Pacemaker Cluster Manager:                        [  OK  ]
```
Снова проверяем /var/log/cluster/corosync.log. Там будет несколько warning по поводу отсутствующей конфигурации /var/lib/pacemaker/cib/cib.xml, но это нормально, мы её сейчас будем создавать.</p>

<p>Проверяем состяние кластера:
```</p>

<h1>crm_mon -1</h1>

<p>Stack: cman
Current DC: node1 - partition with quorum
Version: 1.1.9-1512.el6-2a917dd
2 Nodes configured, unknown expected votes
0 Resources configured.</p>

<p>Online: [ node1 node2 ]
```
Установка закончена. Теперь можно приступать к конфигурированию кластера.</p>

<h3>Конфигурирование pacemaker</h3>

<p>Для управлением конфигурацией кластера будем использовать, как уже раньше отмечалось, crm shell. За разъяснением как работать с pcmk обращайтесь в официальную документацию.</p>

<p>Для начала посмотрим что имеем:
```</p>

<h1>crm configure show</h1>

<p>node node1
node node2
property $id="cib-bootstrap-options" \</p>

<pre><code>    dc-version="1.1.9-1512.el6-2a917dd" \
    cluster-infrastructure="cman"
</code></pre>

<p><code>
Пропишем основные параметры:
</code>
[root@node2 ~]  # crm
crm(live)# configure
crm(live)configure# cib new new_conf
INFO: new_conf shadow CIB created
crm(new_conf)configure# cib use new_conf
crm(new_conf)configure# property stonith-enabled=false
crm(new_conf)configure# property no-quorum-policy=ignore
crm(new_conf)configure# property pe-error-series-max=128
crm(new_conf)configure# property pe-input-series-max=128
crm(new_conf)configure# property pe-warn-series-max=128
crm(new_conf)configure# property symmetric-cluster=false
crm(new_conf)configure# commit
crm(new_conf)configure# cib use live
crm(live)configure# cib commit new_conf
crm(live)configure# show
node node1
node node2
property $id="cib-bootstrap-options" \</p>

<pre><code>    dc-version="1.1.9-1512.el6-2a917dd" \
    cluster-infrastructure="cman" \
    stonith-enabled="false" \
    no-quorum-policy="ignore" \
    pe-error-series-max="128" \
    pe-input-series-max="128" \
    pe-warn-series-max="128" \
    symmetric-cluster="false"
</code></pre>

<p>```
Так, теперь немного комментариев:</p>

<ul>
<li><p><em>crm(live)configure# cib new new_conf</em> - Создаём новую конфигурацию в CIB, в которой и будем делать все изменения. Не рекомендую работать в основной cib (live). Все изменения в новой конфигурации (new_conf) не применяются на кластер до тех пор пока мы её не перенесём в cib live с помощью <em>cib commit new_conf</em>.</p></li>
<li><p><em>property stonith-enabled=false</em> и <em>property no-quorum-policy=ignore</em> - Выключаем fencing и действия по умолчанию при потере кворума.</p></li>
<li><p><em>property symmetric-cluster=false</em> - Переводим кластер в несимметричный режим. В отличии от симметричного, он не запускает нигде никакие ресурсы, если это явно не разрешено в location (об этом ниже). В принципе, в данном случае это нам не нужно, даже добавляет неудобства. Однако это добавляет удобства в конфигурировании если нод больше двух и много ресурсов, которые привязаны только к определённым из них. Я не настаиваю на этом параметре, необходимость в нём зависит от архитектуры кластера.</p></li>
</ul>


<h3>Ресурсы кластера</h3>

<p>Ресурсами в кластере могут являться программы, скрипты, ip адреса, файловые системы и т.д. Вариаций много. Скрипты для управления ресурсами находятся в /usr/lib/ocf/resource.d/. К примеру вот список доступных ресурс агентов для провайдера Heartbeat:
```</p>

<h1>crm ra list ocf heartbeat</h1>

<p>AoEtarget           AudibleAlarm        CTDB                ClusterMon
Delay               Dummy               EvmsSCC             Evmsd
Filesystem          ICP                 IPaddr              IPaddr2
IPsrcaddr           IPv6addr            LVM                 LinuxSCSI
MailTo              ManageRAID          ManageVE            Pure-FTPd
Raid1               Route               SAPDatabase         SAPInstance
SendArp             ServeRAID           SphinxSearchDaemon  Squid
Stateful            SysInfo             VIPArip             VirtualDomain
WAS                 WAS6                WinPopup            Xen
Xinetd              anything            apache              conntrackd
db2                 drbd                eDir88              ethmonitor
exportfs            fio                 iSCSILogicalUnit    iSCSITarget
ids                 iscsi               jboss               lsyncd
lxc                 mysql               mysql-proxy         nfsserver
nginx               oracle              oralsnr             pgsql
pingd               portblock           postfix             proftpd
rsyncd              scsi2reservation    sfex                symlink
syslog-ng           tomcat              vmware
```
Если нет нужного, можно написать свой. Написаны ресурс агенты на bash.­</p>

<p>Приступим к добавлению ресурсов. Создадим к примеру следующую конфигурацию:</p>

<p>Имеется две ноды, на них установлен и настроен nginx, для синхронизации рабочей директории nginx будет использоваться lsyncd/unison. Балансировка будет присходить по двум плавающим ip адресам.</p>

<p>Для начала устанавливаем всё необходимое:
<code>
yum install nginx lsyncd unison
</code>
Про настройку lsyncd я уже <a href="http://sibilia.github.io/blog/2013/03/26/sinkhronizatsiia-failov-s-pomoshchiu-lsyncd-i-unison/">писал</a>. Просто меняем в <em>source</em> и в команде запуска unison /mnt/lsynced на что-нибудь вроде /usr/share/nginx/html. Это конечно зависит от nginx.</p>

<p>Проверяем lsyncd. Запускаем на node2 и создаём файлик на ней же /usr/share/nginx/html/test. В выводе будет что-то вроде:
<code>
[root@node2 ~]  # lsyncd -nodaemon /etc/lsyncd.conf
16:37:59 Normal: Event Create spawns action "/usr/bin/unison -retry 5 -owner -group -batch /usr/share/nginx/html ssh://node1//usr/share/nginx/html"
Contacting server...
Connected [//node1//usr/share/nginx/html -&gt; //node2//usr/share/nginx/html]
...
Synchronization complete  (1 item transferred, 0 skipped, 0 failures)
16:37:59 Normal: Retrying Create on /usr/share/nginx/html//test = 0
</code>
Всё отлично, прерываем lsyncd по ctrl-c.
Ресурс агент для lsyncd я буду использовать свой. Для этого добавляем его на node1 и node2:
<code>
wget -O /usr/lib/ocf/resource.d/heartbeat/lsyncd https://raw.github.com/Sibilia/scripts/master/lsyncd
</code></p>

<p>Настройку nginx приводить не буду, подключайте фантазию.</p>

<p>Теперь приступим к созданию ресурсов в кластере. Для lsyncd я буду использовать clone ресурс, а для nginx два разных ресурса для каждой ноды просто для примера. Выбор того или иного способа описания ресурсов зависит лишь от ситуации и ваших личных предпочтений.
```</p>

<h1>crm</h1>

<p>crm(live)# configure
crm(live)configure# cib use new_conf
crm(new_conf)configure# edit
<code>
И приводим конфигурацию к следующему виде:
</code>
node node1
node node2
primitive ipaddr_215 ocf:heartbeat:IPaddr2 \</p>

<pre><code>    params ip="10.10.0.215"
</code></pre>

<p>primitive ipaddr_216 ocf:heartbeat:IPaddr2 \</p>

<pre><code>    params ip="10.10.0.216"
</code></pre>

<p>primitive lsyncd ocf:heartbeat:lsyncd \</p>

<pre><code>    params cmdline_options="-log scarce"
</code></pre>

<p>primitive nginx1 ocf:heartbeat:nginx
primitive nginx2 ocf:heartbeat:nginx
clone lsyncd-cl lsyncd
location ip_215-on-node1 ipaddr_215 1000: node1
location ip_215-on-node2 ipaddr_215 500: node2
location ip_216-on-node1 ipaddr_216 500: node1
location ip_216-on-node2 ipaddr_216 1000: node2
location lsyncd-on-node1 lsyncd-cl 500: node1
location lsyncd-on-node2 lsyncd-cl 500: node2
location nginx1-on-node1 nginx1 inf: node1
location nginx2-on-node2 nginx2 inf: node2
property $id="cib-bootstrap-options" \</p>

<pre><code>    dc-version="1.1.9-1512.el6-2a917dd" \
    cluster-infrastructure="cman" \
    stonith-enabled="false" \
    no-quorum-policy="ignore" \
    pe-error-series-max="128" \
    pe-input-series-max="128" \
    pe-warn-series-max="128" \
    symmetric-cluster="false"
</code></pre>

<p><code>
После выходим из редактора и применяем конфигурацию на кластер:
</code>
crm(new_conf)configure# commit
crm(new_conf)configure# cib use
crm(live)configure# cib commit new_conf
INFO: commited 'new_conf' shadow CIB to the cluster
crm(live)configure# show
<code>
Теперь проверяем как всё запустилось:
</code></p>

<h1>crm_mon -fnr1</h1>

<p>Stack: cman
Current DC: node2 - partition with quorum
Version: 1.1.9-1512.el6-2a917dd
2 Nodes configured, unknown expected votes
6 Resources configured.</p>

<p>Node node1: online</p>

<pre><code>    nginx1  (ocf::heartbeat:nginx): Started 
    ipaddr_215      (ocf::heartbeat:IPaddr2):       Started 
    lsyncd:1        (ocf::heartbeat:lsyncd):        Started 
</code></pre>

<p>Node node2: online</p>

<pre><code>    nginx2  (ocf::heartbeat:nginx): Started 
    lsyncd:0        (ocf::heartbeat:lsyncd):        Started 
    ipaddr_216      (ocf::heartbeat:IPaddr2):       Started 
</code></pre>

<p>Inactive resources:</p>

<p>Migration summary:
* Node node2:
* Node node1:
<code>
Проверим случай выпадения одной ноды, после её вернём обратно:
</code></p>

<h1>crm node standby node1</h1>

<h1>crm_mon -fnr1</h1>

<p>Stack: cman
Current DC: node2 - partition with quorum
Version: 1.1.9-1512.el6-2a917dd
2 Nodes configured, unknown expected votes
6 Resources configured.</p>

<p>Node node1: standby
Node node2: online</p>

<pre><code>    nginx2  (ocf::heartbeat:nginx): Started 
    ipaddr_215      (ocf::heartbeat:IPaddr2):       Started 
    lsyncd:0        (ocf::heartbeat:lsyncd):        Started 
    ipaddr_216      (ocf::heartbeat:IPaddr2):       Started 
</code></pre>

<p>Inactive resources:</p>

<p> nginx1 (ocf::heartbeat:nginx): Stopped
 Clone Set: lsyncd-cl [lsyncd]</p>

<pre><code> Started: [ node2 ]
 Stopped: [ lsyncd:1 ]
</code></pre>

<p>Migration summary:
* Node node2:
* Node node1:</p>

<h1>crm node online node1</h1>

<p>```
Убедились что оба ip адреса переплыли на одну ноду.</p>

<h3>Работа с кластером</h3>

<p>В рабочем процессе, мы можем управлять состояниями нод и состоянием/размещением ресурсов.
К примеру выведем ноду из кластера и вернём обратно:
```</p>

<h1>crm node standby node1</h1>

<h1>crm node online node1</h1>

<p><code>
Можем остановить/переместить ресурс:
</code></p>

<h1>crm resource move ipaddr_216 node1</h1>

<h1>crm resource stop ipaddr_216</h1>

<h1>crm resource start ipaddr_216</h1>

<h1>crm resource unmove ipaddr_216</h1>

<p><code>
Все эти действия сразу выполняются. Бывает ситуация, когда ресурс вылетает, тогда pacemaker определяет по мониторингу что он не запущен и сново запускает, увеличевая при этом счётчик таких ошибок. К примеру убъём процесс nginx:
</code></p>

<h1>ps aux |grep nginx |grep -v grep</h1>

<p>root     10796  0.0  0.4  96024  2040 ?        Ss   18:48   0:00 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf
nginx    10798  0.0  0.5  96376  2708 ?        S    18:48   0:00 nginx: worker process</p>

<h1>kill -KILL 10796 10798</h1>

<h1>crm_mon -fnr1</h1>

<p>...
Node node1: online</p>

<pre><code>    nginx1  (ocf::heartbeat:nginx): Started 
    ipaddr_215      (ocf::heartbeat:IPaddr2):       Started 
    lsyncd:1        (ocf::heartbeat:lsyncd):        Started 
</code></pre>

<p>Node node2: online</p>

<pre><code>    nginx2  (ocf::heartbeat:nginx): Started 
    ipaddr_216      (ocf::heartbeat:IPaddr2):       Started 
    lsyncd:0        (ocf::heartbeat:lsyncd):        Started 
</code></pre>

<p>Inactive resources:</p>

<p>Migration summary:
* Node node2:
* Node node1:
   nginx1: migration-threshold=1000000 fail-count=2 last-failure='Thu Jun 20 19:17:19 2013'</p>

<p>Failed actions:</p>

<pre><code>nginx1_monitor_5000 (node=node1, call=276, rc=7, status=complete): not running
</code></pre>

<p><code>
Появилась запись что были ошибки. Если же ресурс не может запуститься, или слишком часто вылетает, то значение счётчика fail-count выставляется в 1000000 и после этого этот ресурс на этой ноде не будет востонавливаться. Для сброса счётчика можно сделать:
</code></p>

<h1>crm resource cleanup nginx1</h1>

<p>```</p>

<h3>P.S.</h3>

<p>Это лишь самые базовые возможности Pacemaker. Есть ещё очерёдность запуска (order), совместное размещение (colocation), группы и т.д. Если есть какие замечания, пожелания, неточности в статье, оставляйте в комментариях или пишите на почту.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Синхронизация файлов с помощью Lsyncd и Unison]]></title>
    <link href="http://Sibilia.github.com/blog/2013/03/26/sinkhronizatsiia-failov-s-pomoshchiu-lsyncd-i-unison/"/>
    <updated>2013-03-26T13:52:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/03/26/sinkhronizatsiia-failov-s-pomoshchiu-lsyncd-i-unison</id>
    <content type="html"><![CDATA[<p>Для быстрой синхронизации файлов между двумя серверами, когда изменения могут появиться на любом из них, прекрасно подходит связка из <a href="https://code.google.com/p/lsyncd/">lsyncd</a> и <a href="http://www.cis.upenn.edu/~bcpierce/unison/">unison</a>.</p>

<p>Lsyncd - это демон который слушает дерево каталогов и выполняет синхронизацию при событии (inotify или fsevents) на нём. Синхронизировать он может с помощью rsync или любым другим способом, который можно прописать у него в настройках в виде скрипта на Lua.</p>

<p>Unison позволяет синхронизировать файлы между серверами (или локально два различных каталога). В отличие от rsync он позволяет синхронизировать файлы одновременно в обе стороны. В качестве транспорта при синхронизации может быть использован ssh.
Пример буду приводить для CentOS/RedHat, для .deb систем отличия в мелочах. Начнём с установки:</p>

<!-- more -->


<p><code>
yum install lsyncd unison
</code>
Далее правим конфигурационный файл lsyncd: /etc/lsyncd.conf
<div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>settings <span class="o">{</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    <span class="nv">logfile</span>         <span class="o">=</span> <span class="s2">&quot;/mnt/lsynced.lsync.log&quot;</span>,
</span><span class='line'>    <span class="nv">statusFile</span>      <span class="o">=</span> <span class="s2">&quot;/mnt/lsynced.lsync.status&quot;</span>,
</span><span class='line'>    <span class="nv">statusInterval</span>  <span class="o">=</span> 10,
</span><span class='line'>    <span class="nv">maxDelays</span>       <span class="o">=</span> 3,
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="o">}</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;runUnison <span class="o">=</span> <span class="o">[[</span>/usr/bin/unison -retry 5 -owner -group -batch /mnt/lsynced ssh://node2//mnt/lsynced<span class="o">]]</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;runbash <span class="o">=</span> <span class="o">{</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    <span class="nv">onCreate</span> <span class="o">=</span> runUnison,
</span><span class='line'>    <span class="nv">onDelete</span> <span class="o">=</span> runUnison,
</span><span class='line'>    <span class="nv">onModify</span> <span class="o">=</span> runUnison,
</span><span class='line'>    <span class="nv">onMove</span> <span class="o">=</span> runUnison,
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="o">}</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;sync <span class="o">{</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    runbash,
</span><span class='line'>    <span class="nv">maxProcesses</span> <span class="o">=</span> 1,
</span><span class='line'>    <span class="nv">delay</span> <span class="o">=</span> 3,
</span><span class='line'>    <span class="nb">source</span> <span class="o">=</span> <span class="s2">&quot;/mnt/lsynced&quot;</span>,
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
На втором сервере меняем имя хоста в строке вызова unison. Проверить можно запустив lsyncd в режиме nodaemon (или без этой опции и читать логи):
<code>
lsyncd -nodaemon /etc/lsyncd.conf
</code>
Если всё отлично завершаем и переходим к настройке запуска lsyncd под corosync.
Тут есть два способа: использовать lsb ресурс corosync или написать свой ресурс агент. Я рассмотрю первый вариант. Для второго у меня на github лежит ресурс агент, но я его ещё не до конца протестировал.</p>

<h3>Corosync lsyncd LSB ресурс.</h3>

<p>Нам понадобится скрипт lsyncd daemon: /etc/init.d/lsyncd. Он должен входить в пакет lsyncd начиная с версии 2.1. Учитывая один момент в нём:
<div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">LSYNCD_OPTIONS</span><span class="o">=</span><span class="s2">&quot;-pidfile /var/run/lsyncd.pid /etc/lsyncd.conf&quot;</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;if <span class="o">[</span> -e /etc/sysconfig/lsyncd <span class="o">]</span>; <span class="k">then</span>
</span><span class='line'>  . /etc/sysconfig/lsyncd
</span><span class='line'><span class="k">fi</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
нам обязательно надо прописать опции запуска в файле /etc/sysconfig/lsyncd:
<div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">LSYNCD_OPTIONS</span><span class="o">=</span><span class="s2">&quot;-log scarce /etc/lsyncd.conf&quot;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
Опция "-log scarce" уменьшает уровень логирования.
Теперь осталось добавить lsyncd в конфигурацию corosync:
<code>
crm configure primitive lsyncd-node1 lsb:lsyncd op monitor interval="5" meta target-role="Started"
crm configure location loc-lsyncd-on-node1 lsyncd-node1 inf: node1
</code>
На втором сервере делаем соответствующие поправки.
Вот в принципе и всё. Данное решение синхронизации хорошо себя показало на высоких нагрузках. Если есть вопросы, я постораюсь помочь.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Решение некоторых проблем в DRBD]]></title>
    <link href="http://Sibilia.github.com/blog/2013/01/29/rieshieniie-niekotorykh-probliem-v-drbd/"/>
    <updated>2013-01-29T14:39:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/01/29/rieshieniie-niekotorykh-probliem-v-drbd</id>
    <content type="html"><![CDATA[<p>Рассмотрю кратко решение проблем в DRBD Diskless и Split-brain.</p>

<h3>DRBD Diskless</h3>

<p>При выходе из строя дискового массива и его восстановления  можно получить следующую ситуацию в drbd:</p>

<pre><code># drbd-overview
2:r2 Connected Secondary/Primary Diskless/UpToDate C r----
</code></pre>

<h4>Для решения этой проблемы необходимо сбросить мета данные:</h4>

<p>На активной ноде необходимо отмонтировать раздел. Затем на неактивной ноде отключаем ресурс:</p>

<pre><code># drbdadm down r2
</code></pre>

<p>Создаем заново блок мета-данных:</p>

<pre><code># drbdadm create-md r2
 ...
New drbd meta data block successfully created.
</code></pre>

<!-- more -->


<p>Включаем ресурс обратно (должна начаться синхронизация):</p>

<pre><code># drbdadm up r2
# drbdadm connect r2
# drbd-overview 
2:r2 SyncTarget Secondary/Primary Inconsistent/UpToDate C r---- 
[&gt;....................] sync'ed: 0.1% (31796/31796)M queue_delay: 0.0 ms 
</code></pre>

<p>Для изменения скорости синхронизации можно ввести:</p>

<pre><code># drbdsetup /dev/drbd2 syncer -r 10M
</code></pre>

<p>Для задания в настройках необходимо прописать в /etc/drbd.conf :
```
syncer {</p>

<pre><code>rate 100M;
</code></pre>

<p>}
```</p>

<h3>DRBD Split-brain</h3>

<p>Ещё бывает ситуация когда ноды не синхронизируются со следующими признаками:</p>

<pre><code># drbd-overview
3:just StandAlone Primary/Unknown UpToDate/DUnknown r----- /mnt/Just ext3 5.0G 156M 4.6G 4%
</code></pre>

<p>Причиной этого может стать состояние split-brain. Для решения этой проблемы необходимо:
На secondary:</p>

<pre><code># drbdadm disconnect just
# drbdadm -- --discard-my-data connect just
</code></pre>

<p>На primary:</p>

<pre><code># drbdadm connect just
</code></pre>

<p>После синхронизации (если она необходима) всё должно работать:</p>

<pre><code># drbd-overview
3:just Connected Primary/Secondary UpToDate/UpToDate C r----- /mnt/Just ext3 5.0G 156M 4.6G 4%
</code></pre>
]]></content>
  </entry>
  
</feed>
