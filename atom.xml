<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Sibilia octopress blog]]></title>
  <link href="http://Sibilia.github.com/atom.xml" rel="self"/>
  <link href="http://Sibilia.github.com/"/>
  <updated>2013-06-21T15:41:13+03:00</updated>
  <id>http://Sibilia.github.com/</id>
  <author>
    <name><![CDATA[Ilia Sibiryatkin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Установка и настройка кластера Pacemaker]]></title>
    <link href="http://Sibilia.github.com/blog/2013/06/20/ustanovka-i-nastroika-pacemaker/"/>
    <updated>2013-06-20T12:41:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/06/20/ustanovka-i-nastroika-pacemaker</id>
    <content type="html"><![CDATA[<p>Для построения кластеров high availability (высокой доступности) я в своей работе использую <a href="http://clusterlabs.org/">Pacemaker</a>. Он себя хорошо зарекомендовал на многих проектах, очень гибкий, динамически развивается. Здесь я приведу краткую инструкцию себе на память по его установке и начальной настройке. Я не претендую на полноту изложения, за этим лучше идти в официальную <a href="http://clusterlabs.org/doc/">документацию</a>, благо она неплохо написана. Используемый дистрибутив CentOS 6. Пример будет описан на двух однотипных серверах.</p>

<!--more-->


<h3>Подготовка</h3>

<p>Описывать установку CentOS я не буду, этого материала хватает. Я рекомендую устанавливать минимальный netinstall, всё что нужно сами доставим.</p>

<p>Вкратце опишу что нам нужно перед тем как приступить непосредственно к установке pacemaker:</p>

<ul>
<li>Установленная CentOS.</li>
<li>Настроенная сеть:  сервера должны быть в одной сети (по крайней мере для этого примера).</li>
<li>hostname прописаны в /etc/hosts (как свой так и соседа). В данном примере будут использоваться имена <strong>node1</strong> и <strong>node2</strong>.</li>
<li>Установлен и настроен SSH. Сгенерированы RSA ключи и разнесены по нодам. Доступ между node1 и node2 по ssh осуществляется без пароля. Есть в принципе способ генерировать ключи непосредственно внутри pacemaker, но этот способ я не рассматриваю, если интересно то вам нужно после установки кластера запустить <em>corosync-keygen</em> и разнести файл /etc/corosync/authkey по нодам.</li>
<li>Установлен и настроен NTP.</li>
</ul>


<p>Устанавливать будем из репозитрия pacemaker - <a href="http://clusterlabs.org/rpm-next/">clusterlabs</a>, там стабильная версия и более свежая по сравнение с репозиторием CentOS. Добавляем репозиторий.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>wget -O /etc/yum.repos.d/pacemaker.repo http://clusterlabs.org/rpm-next/rhel-6/clusterlabs.repo</span></code></pre></td></tr></table></div></figure>


<p>Для работы с pacemaker я использую crm shell. К моему сожалению он теперь поставляется отдельно, добавим репозиторий для его установки. Можно его не ставить, тогда можно работать с pcmk, но мне удобен больше crm shell.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>wget -O /etc/yum.repos.d/ha-clustering.repo http://download.opensuse.org/repositories/network:/ha-clustering/CentOS_CentOS-6/network:ha-clustering.repo</span></code></pre></td></tr></table></div></figure>


<h3>Установка</h3>

<p>Текущая стабильная версия pacemaker использует архитектуру CMAN (вместо openais). Вообще, история развития кластера pacemaker/corosync очень багатая на глобальные изменения, но это отделная тема. Если интересна история развития кластеров то могу порекомендовать вот эту <a href="http://mlug.linux.by/?p=413">презентацию</a> от Владислава Богданова, ему кстати большая благодарность.</p>

<p>Приступим к установке:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># yum install pacemaker cman crmsh ccs</span></code></pre></td></tr></table></div></figure>


<p>После завершения установки, у меня были следующие версии основных пакетов:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># rpm -qa |grep -e pacemaker -e cman -e corosync -e crmsh
</span><span class='line'>pacemaker-cli-1.1.9-1512.el6.x86_64
</span><span class='line'>pacemaker-1.1.9-1512.el6.x86_64
</span><span class='line'>corosync-1.4.5-35.1.x86_64
</span><span class='line'>pacemaker-libs-1.1.9-1512.el6.x86_64
</span><span class='line'>pacemaker-cluster-libs-1.1.9-1512.el6.x86_64
</span><span class='line'>cman-3.0.12.1-49.el6.x86_64
</span><span class='line'>corosynclib-1.4.5-35.1.x86_64
</span><span class='line'>crmsh-1.2.5-55.6.x86_64</span></code></pre></td></tr></table></div></figure>


<p>Копируем пример конфига:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cp /etc/corosync/corosync.conf.example.udpu /etc/corosync/corosync.conf</span></code></pre></td></tr></table></div></figure>


<p>Я использую для связи между нодами клсатера unicast вместо multicast, на сеть нагрузки меньше, защита лучше. Редактируем конфиг для нашей ситауции. Для тонкой настройки рекомендую почитать <em>man corosync.conf</em>. Привожу конфиг /etc/corosync/corosync.conf:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Please read the corosync.conf.5 manual page
</span><span class='line'>compatibility: whitetank
</span><span class='line'>
</span><span class='line'>totem {
</span><span class='line'>        version: 2
</span><span class='line'>        secauth: off
</span><span class='line'>        threads: 0
</span><span class='line'>        transport: udpu
</span><span class='line'>        interface {
</span><span class='line'>                member {
</span><span class='line'>                        memberaddr: 10.10.0.210
</span><span class='line'>                }
</span><span class='line'>                member {
</span><span class='line'>                        memberaddr: 10.10.0.211
</span><span class='line'>                }
</span><span class='line'>                ringnumber: 0
</span><span class='line'>                bindnetaddr: 10.10.0.0
</span><span class='line'>                mcastaddr: 226.98.1.1
</span><span class='line'>                mcastport: 5500
</span><span class='line'>                ttl: 1
</span><span class='line'>        }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>logging {
</span><span class='line'>        fileline: off
</span><span class='line'>        to_stderr: no
</span><span class='line'>        to_logfile: yes
</span><span class='line'>        to_syslog: yes
</span><span class='line'>        logfile: /var/log/cluster/corosync.log
</span><span class='line'>        debug: on
</span><span class='line'>        timestamp: on
</span><span class='line'>        logger_subsys {
</span><span class='line'>                subsys: AMF
</span><span class='line'>                debug: off
</span><span class='line'>        }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>amf {
</span><span class='line'>        mode: disabled
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Так же не забываем включить сервис Pacemaker:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># cat &lt;&lt; END &gt;&gt; /etc/corosync/service.d/pcmk
</span><span class='line'>service {
</span><span class='line'>        name: pacemaker
</span><span class='line'>        ver:  1
</span><span class='line'>}
</span><span class='line'>END</span></code></pre></td></tr></table></div></figure>


<p>По поводу значения <em>ver:  1</em>, он указывает что сервис pacemaker мы будем запускать сами. Для начала оставим так, в дальнейшем его можно поменять на <em>ver:  0</em> тогда pacemaker будет запускаться сам после запуска кластера.</p>

<p>Для CMAN необходим ещё один конфигурационный файл /etc/cluster/cluster.conf. Он имеет формат xml. Писать ручками его не практично, будем использовать установленную ранее <em>ccs</em>. Для начала пропишем имя кластеру и добавим ноды:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># ccs -f /etc/cluster/cluster.conf --createcluster my_cluster
</span><span class='line'># ccs -f /etc/cluster/cluster.conf --addnode node1
</span><span class='line'>Node node1 added.
</span><span class='line'># ccs -f /etc/cluster/cluster.conf --addnode node2
</span><span class='line'>Node node2 added.</span></code></pre></td></tr></table></div></figure>


<p>Затем, как написано в документации pacemaker:</p>

<blockquote><p>Далее нам нужно научить CMAN как посылать запросы fencing для pacemaker. Мы прописываем это независимо от того, включен fencing в pacemaker, или нет.</p></blockquote>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># ccs -f /etc/cluster/cluster.conf --addfencedev pcmk agent=fence_pcmk
</span><span class='line'># ccs -f /etc/cluster/cluster.conf --addmethod pcmk-redirect node1
</span><span class='line'>Method pcmk-redirect added to node1.
</span><span class='line'># ccs -f /etc/cluster/cluster.conf --addmethod pcmk-redirect node2
</span><span class='line'>Method pcmk-redirect added to node2.
</span><span class='line'># ccs -f /etc/cluster/cluster.conf --addfenceinst pcmk node1 pcmk-redirect port=node1
</span><span class='line'># ccs -f /etc/cluster/cluster.conf --addfenceinst pcmk node2 pcmk-redirect port=node2</span></code></pre></td></tr></table></div></figure>


<p>После смотрим что у нас получилось, проверяем синтаксис с помощью <em>ccs_config_validate</em> и копируем этот конфиг на вторую ноду.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># cat /etc/cluster/cluster.conf
</span><span class='line'>&lt;cluster config_version="8" name="my_cluster"&gt;
</span><span class='line'>  &lt;fence_daemon/&gt;
</span><span class='line'>  &lt;clusternodes&gt;
</span><span class='line'>    &lt;clusternode name="node1" nodeid="1"&gt;
</span><span class='line'>      &lt;fence&gt;
</span><span class='line'>        &lt;method name="pcmk-redirect"&gt;
</span><span class='line'>          &lt;device name="pcmk" port="node1"/&gt;
</span><span class='line'>        &lt;/method&gt;
</span><span class='line'>      &lt;/fence&gt;
</span><span class='line'>    &lt;/clusternode&gt;
</span><span class='line'>    &lt;clusternode name="node2" nodeid="2"&gt;
</span><span class='line'>      &lt;fence&gt;
</span><span class='line'>        &lt;method name="pcmk-redirect"&gt;
</span><span class='line'>          &lt;device name="pcmk" port="node2"/&gt;
</span><span class='line'>        &lt;/method&gt;
</span><span class='line'>      &lt;/fence&gt;
</span><span class='line'>    &lt;/clusternode&gt;
</span><span class='line'>  &lt;/clusternodes&gt;
</span><span class='line'>  &lt;cman/&gt;
</span><span class='line'>  &lt;fencedevices&gt;
</span><span class='line'>    &lt;fencedevice agent="fence_pcmk" name="pcmk"/&gt;
</span><span class='line'>  &lt;/fencedevices&gt;
</span><span class='line'>  &lt;rm&gt;
</span><span class='line'>    &lt;failoverdomains/&gt;
</span><span class='line'>    &lt;resources/&gt;
</span><span class='line'>  &lt;/rm&gt;
</span><span class='line'>&lt;/cluster&gt;
</span><span class='line'># ccs_config_validate
</span><span class='line'>Configuration validates
</span><span class='line'># scp /etc/cluster/cluster.conf node2:/etc/cluster/cluster.conf</span></code></pre></td></tr></table></div></figure>


<p>Всё отлично. Повторяем все предыдущие шаги на node2. Теперь мы имеем подготовленные node1 и node2.</p>

<p>Теперь можно запускать кластер, но перед этим ещё одно замечание.</p>

<p>Первоначально, CMAN была написана для rgmanager и предполагает, что кластер не должен запускаться, пока нода не имеет кворума. Поэтому прежде чем пытаться запустить кластер, мы отключим эту особеннсть. Выполняем на node1 и node2:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># echo "CMAN_QUORUM_TIMEOUT=0" &gt;&gt; /etc/sysconfig/cman</span></code></pre></td></tr></table></div></figure>


<p>Теперь пробуем запустить кластер. Выполняем на node1 и node2:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># service cman start
</span><span class='line'>Starting cluster: 
</span><span class='line'>   Checking if cluster has been disabled at boot...        [  OK  ]
</span><span class='line'>   Checking Network Manager...                             [  OK  ]
</span><span class='line'>   Global setup...                                         [  OK  ]
</span><span class='line'>   Loading kernel modules...                               [  OK  ]
</span><span class='line'>   Mounting configfs...                                    [  OK  ]
</span><span class='line'>   Starting cman...                                        [  OK  ]
</span><span class='line'>   Waiting for quorum...                                   [  OK  ]
</span><span class='line'>   Starting fenced...                                      [  OK  ]
</span><span class='line'>   Starting dlm_controld...                                [  OK  ]
</span><span class='line'>   Tuning DLM kernel config...                             [  OK  ]
</span><span class='line'>   Starting gfs_controld...                                [  OK  ]
</span><span class='line'>   Unfencing self...                                       [  OK  ]
</span><span class='line'>   Joining fence domain...                                 [  OK  ]</span></code></pre></td></tr></table></div></figure>


<p>Проверяем ноды:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># cman_tool nodes
</span><span class='line'>Node  Sts   Inc   Joined               Name
</span><span class='line'>   1   M     44   2013-06-20 15:20:20  node1
</span><span class='line'>   2   M     40   2013-06-20 15:20:20  node2</span></code></pre></td></tr></table></div></figure>


<p>Вроде всё чисто, проверяем в /var/log/cluster/corosync.log. Ищем там любые warning и error. Если и там всё отлично, пробуем стартовать pacemaker. Опять же запускаем на node1 и node2:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># service pacemaker start
</span><span class='line'>Starting cluster: 
</span><span class='line'>   Checking if cluster has been disabled at boot...        [  OK  ]
</span><span class='line'>   Checking Network Manager...                             [  OK  ]
</span><span class='line'>   Global setup...                                         [  OK  ]
</span><span class='line'>   Loading kernel modules...                               [  OK  ]
</span><span class='line'>   Mounting configfs...                                    [  OK  ]
</span><span class='line'>   Starting cman...                                        [  OK  ]
</span><span class='line'>   Waiting for quorum...                                   [  OK  ]
</span><span class='line'>   Starting fenced...                                      [  OK  ]
</span><span class='line'>   Starting dlm_controld...                                [  OK  ]
</span><span class='line'>   Tuning DLM kernel config...                             [  OK  ]
</span><span class='line'>   Starting gfs_controld...                                [  OK  ]
</span><span class='line'>   Unfencing self...                                       [  OK  ]
</span><span class='line'>   Joining fence domain...                                 [  OK  ]
</span><span class='line'>Starting Pacemaker Cluster Manager:                        [  OK  ]</span></code></pre></td></tr></table></div></figure>


<p>Снова проверяем /var/log/cluster/corosync.log. Там будет несколько warning по поводу отсутствующей конфигурации /var/lib/pacemaker/cib/cib.xml, но это нормально, мы её сейчас будем создавать.</p>

<p>Проверяем состяние кластера:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># crm_mon -1
</span><span class='line'>Stack: cman
</span><span class='line'>Current DC: node1 - partition with quorum
</span><span class='line'>Version: 1.1.9-1512.el6-2a917dd
</span><span class='line'>2 Nodes configured, unknown expected votes
</span><span class='line'>0 Resources configured.
</span><span class='line'>
</span><span class='line'>Online: [ node1 node2 ]</span></code></pre></td></tr></table></div></figure>


<p>Установка закончена. Теперь можно приступать к конфигурированию кластера.</p>

<h3>Конфигурирование pacemaker</h3>

<p>Для управлением конфигурацией кластера будем использовать, как уже раньше отмечалось, crm shell. За разъяснением как работать с pcmk обращайтесь в официальную документацию.</p>

<p>Для начала посмотрим что имеем:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># crm configure show
</span><span class='line'>node node1
</span><span class='line'>node node2
</span><span class='line'>property $id="cib-bootstrap-options" \
</span><span class='line'>        dc-version="1.1.9-1512.el6-2a917dd" \
</span><span class='line'>        cluster-infrastructure="cman"</span></code></pre></td></tr></table></div></figure>


<p>Пропишем основные параметры:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@node2 ~]  # crm
</span><span class='line'>crm(live)# configure 
</span><span class='line'>crm(live)configure# cib new new_conf
</span><span class='line'>INFO: new_conf shadow CIB created
</span><span class='line'>crm(new_conf)configure# cib use new_conf 
</span><span class='line'>crm(new_conf)configure# property stonith-enabled=false
</span><span class='line'>crm(new_conf)configure# property no-quorum-policy=ignore
</span><span class='line'>crm(new_conf)configure# property pe-error-series-max=128
</span><span class='line'>crm(new_conf)configure# property pe-input-series-max=128
</span><span class='line'>crm(new_conf)configure# property pe-warn-series-max=128
</span><span class='line'>crm(new_conf)configure# property symmetric-cluster=false
</span><span class='line'>crm(new_conf)configure# commit 
</span><span class='line'>crm(new_conf)configure# cib use live
</span><span class='line'>crm(live)configure# cib commit new_conf
</span><span class='line'>crm(live)configure# show 
</span><span class='line'>node node1
</span><span class='line'>node node2
</span><span class='line'>property $id="cib-bootstrap-options" \
</span><span class='line'>        dc-version="1.1.9-1512.el6-2a917dd" \
</span><span class='line'>        cluster-infrastructure="cman" \
</span><span class='line'>        stonith-enabled="false" \
</span><span class='line'>        no-quorum-policy="ignore" \
</span><span class='line'>        pe-error-series-max="128" \
</span><span class='line'>        pe-input-series-max="128" \
</span><span class='line'>        pe-warn-series-max="128" \
</span><span class='line'>        symmetric-cluster="false"</span></code></pre></td></tr></table></div></figure>


<p>Так, теперь немного комментариев:</p>

<ul>
<li><p><em>crm(live)configure# cib new new_conf</em> - Создаём новую конфигурацию в CIB, в которой и будем делать все изменения. Не рекомендую работать в основной cib (live). Все изменения в новой конфигурации (new_conf) не применяются на кластер до тех пор пока мы её не перенесём в cib live с помощью <em>cib commit new_conf</em>.</p></li>
<li><p><em>property stonith-enabled=false</em> и <em>property no-quorum-policy=ignore</em> - Выключаем fencing и действия по умолчанию при потере кворума.</p></li>
<li><p><em>property symmetric-cluster=false</em> - Переводим кластер в несимметричный режим. В отличии от симметричного, он не запускает нигде никакие ресурсы, если это явно не разрешено в location (об этом ниже). В принципе, в данном случае это нам не нужно, даже добавляет неудобства. Однако это добавляет удобства в конфигурировании если нод больше двух и много ресурсов, которые привязаны только к определённым из них. Я не настаиваю на этом параметре, необходимость в нём зависит от архитектуры кластера.</p></li>
</ul>


<h3>Ресурсы кластера</h3>

<p>Ресурсами в кластере могут являться программы, скрипты, ip адреса, файловые системы и т.д. Вариаций много. Скрипты для управления ресурсами находятся в /usr/lib/ocf/resource.d/. К примеру вот список доступных ресурс агентов для провайдера Heartbeat:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># crm ra list ocf heartbeat
</span><span class='line'>AoEtarget           AudibleAlarm        CTDB                ClusterMon
</span><span class='line'>Delay               Dummy               EvmsSCC             Evmsd
</span><span class='line'>Filesystem          ICP                 IPaddr              IPaddr2
</span><span class='line'>IPsrcaddr           IPv6addr            LVM                 LinuxSCSI
</span><span class='line'>MailTo              ManageRAID          ManageVE            Pure-FTPd
</span><span class='line'>Raid1               Route               SAPDatabase         SAPInstance
</span><span class='line'>SendArp             ServeRAID           SphinxSearchDaemon  Squid
</span><span class='line'>Stateful            SysInfo             VIPArip             VirtualDomain
</span><span class='line'>WAS                 WAS6                WinPopup            Xen
</span><span class='line'>Xinetd              anything            apache              conntrackd
</span><span class='line'>db2                 drbd                eDir88              ethmonitor
</span><span class='line'>exportfs            fio                 iSCSILogicalUnit    iSCSITarget
</span><span class='line'>ids                 iscsi               jboss               lsyncd
</span><span class='line'>lxc                 mysql               mysql-proxy         nfsserver
</span><span class='line'>nginx               oracle              oralsnr             pgsql
</span><span class='line'>pingd               portblock           postfix             proftpd
</span><span class='line'>rsyncd              scsi2reservation    sfex                symlink
</span><span class='line'>syslog-ng           tomcat              vmware</span></code></pre></td></tr></table></div></figure>


<p>Если нет нужного, можно написать свой. Написаны ресурс агенты на bash.­</p>

<p>Приступим к добавлению ресурсов. Создадим к примеру следующую конфигурацию:</p>

<p>Имеется две ноды, на них установлен и настроен nginx, для синхронизации рабочей директории nginx будет использоваться lsyncd/unison. Балансировка будет присходить по двум плавающим ip адресам.</p>

<p>Для начала устанавливаем всё необходимое:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install nginx lsyncd unison</span></code></pre></td></tr></table></div></figure>


<p>Про настройку lsyncd я уже <a href="http://sibilia.github.io/blog/2013/03/26/sinkhronizatsiia-failov-s-pomoshchiu-lsyncd-i-unison/">писал</a>. Просто меняем в <em>source</em> и в команде запуска unison /mnt/lsynced на что-нибудь вроде /usr/share/nginx/html. Это конечно зависит от nginx.</p>

<p>Проверяем lsyncd. Запускаем на node2 и создаём файлик на ней же /usr/share/nginx/html/test. В выводе будет что-то вроде:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@node2 ~]  # lsyncd -nodaemon /etc/lsyncd.conf
</span><span class='line'>16:37:59 Normal: Event Create spawns action "/usr/bin/unison -retry 5 -owner -group -batch /usr/share/nginx/html ssh://node1//usr/share/nginx/html"
</span><span class='line'>Contacting server...
</span><span class='line'>Connected [//node1//usr/share/nginx/html -&gt; //node2//usr/share/nginx/html]
</span><span class='line'>...
</span><span class='line'>Synchronization complete  (1 item transferred, 0 skipped, 0 failures)
</span><span class='line'>16:37:59 Normal: Retrying Create on /usr/share/nginx/html//test = 0</span></code></pre></td></tr></table></div></figure>


<p>Всё отлично, прерываем lsyncd по ctrl-c.
Ресурс агент для lsyncd я буду использовать свой. Для этого добавляем его на node1 и node2:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>wget -O /usr/lib/ocf/resource.d/heartbeat/lsyncd https://raw.github.com/Sibilia/scripts/master/lsyncd</span></code></pre></td></tr></table></div></figure>


<p>Настройку nginx приводить не буду, подключайте фантазию.</p>

<p>Теперь приступим к созданию ресурсов в кластере. Для lsyncd я буду использовать clone ресурс, а для nginx два разных ресурса для каждой ноды просто для примера. Выбор того или иного способа описания ресурсов зависит лишь от ситуации и ваших личных предпочтений.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># crm
</span><span class='line'>crm(live)# configure 
</span><span class='line'>crm(live)configure# cib use new_conf
</span><span class='line'>crm(new_conf)configure# edit</span></code></pre></td></tr></table></div></figure>


<p>И приводим конфигурацию к следующему виде:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>node node1
</span><span class='line'>node node2
</span><span class='line'>primitive ipaddr_215 ocf:heartbeat:IPaddr2 \
</span><span class='line'>        params ip="10.10.0.215"
</span><span class='line'>primitive ipaddr_216 ocf:heartbeat:IPaddr2 \
</span><span class='line'>        params ip="10.10.0.216"
</span><span class='line'>primitive lsyncd ocf:heartbeat:lsyncd \
</span><span class='line'>        params cmdline_options="-log scarce"
</span><span class='line'>primitive nginx1 ocf:heartbeat:nginx
</span><span class='line'>primitive nginx2 ocf:heartbeat:nginx
</span><span class='line'>clone lsyncd-cl lsyncd
</span><span class='line'>location ip_215-on-node1 ipaddr_215 1000: node1
</span><span class='line'>location ip_215-on-node2 ipaddr_215 500: node2
</span><span class='line'>location ip_216-on-node1 ipaddr_216 500: node1
</span><span class='line'>location ip_216-on-node2 ipaddr_216 1000: node2
</span><span class='line'>location lsyncd-on-node1 lsyncd-cl 500: node1
</span><span class='line'>location lsyncd-on-node2 lsyncd-cl 500: node2
</span><span class='line'>location nginx1-on-node1 nginx1 inf: node1
</span><span class='line'>location nginx2-on-node2 nginx2 inf: node2
</span><span class='line'>property $id="cib-bootstrap-options" \
</span><span class='line'>        dc-version="1.1.9-1512.el6-2a917dd" \
</span><span class='line'>        cluster-infrastructure="cman" \
</span><span class='line'>        stonith-enabled="false" \
</span><span class='line'>        no-quorum-policy="ignore" \
</span><span class='line'>        pe-error-series-max="128" \
</span><span class='line'>        pe-input-series-max="128" \
</span><span class='line'>        pe-warn-series-max="128" \
</span><span class='line'>        symmetric-cluster="false"</span></code></pre></td></tr></table></div></figure>


<p>После выходим из редактора и применяем конфигурацию на кластер:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>crm(new_conf)configure# commit
</span><span class='line'>crm(new_conf)configure# cib use
</span><span class='line'>crm(live)configure# cib commit new_conf 
</span><span class='line'>INFO: commited 'new_conf' shadow CIB to the cluster
</span><span class='line'>crm(live)configure# show</span></code></pre></td></tr></table></div></figure>


<p>Теперь проверяем как всё запустилось:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># crm_mon -fnr1
</span><span class='line'>Stack: cman
</span><span class='line'>Current DC: node2 - partition with quorum
</span><span class='line'>Version: 1.1.9-1512.el6-2a917dd
</span><span class='line'>2 Nodes configured, unknown expected votes
</span><span class='line'>6 Resources configured.
</span><span class='line'>
</span><span class='line'>Node node1: online
</span><span class='line'>        nginx1  (ocf::heartbeat:nginx): Started 
</span><span class='line'>        ipaddr_215      (ocf::heartbeat:IPaddr2):       Started 
</span><span class='line'>        lsyncd:1        (ocf::heartbeat:lsyncd):        Started 
</span><span class='line'>Node node2: online
</span><span class='line'>        nginx2  (ocf::heartbeat:nginx): Started 
</span><span class='line'>        lsyncd:0        (ocf::heartbeat:lsyncd):        Started 
</span><span class='line'>        ipaddr_216      (ocf::heartbeat:IPaddr2):       Started 
</span><span class='line'>
</span><span class='line'>Inactive resources:
</span><span class='line'>
</span><span class='line'>Migration summary:
</span><span class='line'>* Node node2: 
</span><span class='line'>* Node node1: </span></code></pre></td></tr></table></div></figure>


<p>Проверим случай выпадения одной ноды, после её вернём обратно:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># crm node standby node1
</span><span class='line'># crm_mon -fnr1
</span><span class='line'>Stack: cman
</span><span class='line'>Current DC: node2 - partition with quorum
</span><span class='line'>Version: 1.1.9-1512.el6-2a917dd
</span><span class='line'>2 Nodes configured, unknown expected votes
</span><span class='line'>6 Resources configured.
</span><span class='line'>
</span><span class='line'>Node node1: standby
</span><span class='line'>Node node2: online
</span><span class='line'>        nginx2  (ocf::heartbeat:nginx): Started 
</span><span class='line'>        ipaddr_215      (ocf::heartbeat:IPaddr2):       Started 
</span><span class='line'>        lsyncd:0        (ocf::heartbeat:lsyncd):        Started 
</span><span class='line'>        ipaddr_216      (ocf::heartbeat:IPaddr2):       Started 
</span><span class='line'>
</span><span class='line'>Inactive resources:
</span><span class='line'>
</span><span class='line'> nginx1 (ocf::heartbeat:nginx): Stopped 
</span><span class='line'> Clone Set: lsyncd-cl [lsyncd]
</span><span class='line'>     Started: [ node2 ]
</span><span class='line'>     Stopped: [ lsyncd:1 ]
</span><span class='line'>
</span><span class='line'>Migration summary:
</span><span class='line'>* Node node2: 
</span><span class='line'>* Node node1:
</span><span class='line'>
</span><span class='line'># crm node online node1</span></code></pre></td></tr></table></div></figure>


<p>Убедились что оба ip адреса переплыли на одну ноду.</p>

<h3>Работа с кластером</h3>

<p>В рабочем процессе, мы можем управлять состояниями нод и состоянием/размещением ресурсов.
К примеру выведем ноду из кластера и вернём обратно:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># crm node standby node1
</span><span class='line'># crm node online node1</span></code></pre></td></tr></table></div></figure>


<p>Можем остановить/переместить ресурс:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># crm resource move ipaddr_216 node1
</span><span class='line'># crm resource stop ipaddr_216
</span><span class='line'># crm resource start ipaddr_216
</span><span class='line'># crm resource unmove ipaddr_216</span></code></pre></td></tr></table></div></figure>


<p>Все эти действия сразу выполняются. Бывает ситуация, когда ресурс вылетает, тогда pacemaker определяет по мониторингу что он не запущен и сново запускает, увеличевая при этом счётчик таких ошибок. К примеру убъём процесс nginx:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># ps aux |grep nginx |grep -v grep
</span><span class='line'>root     10796  0.0  0.4  96024  2040 ?        Ss   18:48   0:00 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf
</span><span class='line'>nginx    10798  0.0  0.5  96376  2708 ?        S    18:48   0:00 nginx: worker process                   
</span><span class='line'># kill -KILL 10796 10798
</span><span class='line'># crm_mon -fnr1
</span><span class='line'>...
</span><span class='line'>Node node1: online
</span><span class='line'>        nginx1  (ocf::heartbeat:nginx): Started 
</span><span class='line'>        ipaddr_215      (ocf::heartbeat:IPaddr2):       Started 
</span><span class='line'>        lsyncd:1        (ocf::heartbeat:lsyncd):        Started 
</span><span class='line'>Node node2: online
</span><span class='line'>        nginx2  (ocf::heartbeat:nginx): Started 
</span><span class='line'>        ipaddr_216      (ocf::heartbeat:IPaddr2):       Started 
</span><span class='line'>        lsyncd:0        (ocf::heartbeat:lsyncd):        Started 
</span><span class='line'>
</span><span class='line'>Inactive resources:
</span><span class='line'>
</span><span class='line'>Migration summary:
</span><span class='line'>* Node node2: 
</span><span class='line'>* Node node1: 
</span><span class='line'>   nginx1: migration-threshold=1000000 fail-count=2 last-failure='Thu Jun 20 19:17:19 2013'
</span><span class='line'>
</span><span class='line'>Failed actions:
</span><span class='line'>    nginx1_monitor_5000 (node=node1, call=276, rc=7, status=complete): not running</span></code></pre></td></tr></table></div></figure>


<p>Появилась запись что были ошибки. Если же ресурс не может запуститься, или слишком часто вылетает, то значение счётчика fail-count выставляется в 1000000 и после этого этот ресурс на этой ноде не будет востонавливаться. Для сброса счётчика можно сделать:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># crm resource cleanup nginx1</span></code></pre></td></tr></table></div></figure>


<h3>P.S.</h3>

<p>Это лишь самые базовые возможности Pacemaker. Есть ещё очерёдность запуска (order), совместное размещение (colocation), группы и т.д. Если есть какие замечания, пожелания, неточности в статье, оставляйте в комментариях или пишите на почту.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SSH трюки]]></title>
    <link href="http://Sibilia.github.com/blog/2013/05/28/ssh-trick/"/>
    <updated>2013-05-28T16:17:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/05/28/ssh-trick</id>
    <content type="html"><![CDATA[<h3>Проксирование соединения</h3>

<p>Допустим у вас есть необходимость подключаться к удалённому серверу host-end, к которому прямого доступа нет, и есть ssh доступ к серверу host-forw, с которого есть доступ к host-end. Тогда для удобства можно настроить проксирование ssh соединения. В результате, вместо последвательного подключения к host-forw -> host-end, можно будет подключаться сразу к host-end и соединение будет проксироваться через host-forw автоматически.
Для этого, нужно дбавить в конфиг ssh (~/.ssh/config):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ForwardAgent yes
</span><span class='line'>Host host-end
</span><span class='line'>    HostName host-end
</span><span class='line'>    Port 22
</span><span class='line'>    User admin
</span><span class='line'>    ProxyCommand ssh admin@host-forw nc %h %p
</span><span class='line'>    Compression yes
</span><span class='line'>    ForwardX11 no</span></code></pre></td></tr></table></div></figure>


<p>Из примера видно, что можно все необходимые параметры прописать, а их кстати немало (man ssh_config).
Теперь для подключения достаточно набрать:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ssh host-end</span></code></pre></td></tr></table></div></figure>


<h3>Просмотр сетевого трафика с удалённого сервера</h3>

<p>На удалённом сервере зачастую есть сетевой сниффер tcpdump. Но снимать трафик на нём, затем переносить на свой комп для анализа в wireshark неудобно, намного проще направить поток с tcpdump сразу себе в wireshark:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ssh root@host-end -i eth0 -w - 'port !22' | wireshark -k -i -</span></code></pre></td></tr></table></div></figure>


<h3>Копирование файлов с удалённого сервера на другой, через локальный комп</h3>

<p>Необходимо перекинуть файлы с одного сервера на другой, но они друг друга не видят. Тогда можем перекинуть с локального, котой их обоих видит:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ssh root@host1 "tar -cf - /dir-copy" | ssh root@host2 "tar -xf - /dir-past/"</span></code></pre></td></tr></table></div></figure>


<h3>Запуск локального скрипта на удалённом сервере</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ssh -T user@host &lt; script.sh</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NTP - синхронизация времени]]></title>
    <link href="http://Sibilia.github.com/blog/2013/04/11/ntp-sinkhronizatsiia-vriemieni/"/>
    <updated>2013-04-11T14:23:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/04/11/ntp-sinkhronizatsiia-vriemieni</id>
    <content type="html"><![CDATA[<p>Настроить синхронизацию времени в CentOS не сложно. Для этого нам понадобится NTP. Устанавливаем его:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install ntp</span></code></pre></td></tr></table></div></figure>


<p>Далее разрешаем демону ntpd устанавливать аппаратное время. Для этого в файле /etc/sysconfig/ntpd прописываем строку SYNC_HWCLOCK=yes :</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>echo SYNC_HWCLOCK=yes &gt;&gt; /etc/sysconfig/ntpd</span></code></pre></td></tr></table></div></figure>


<p>Запускаем демон ntpd и добавляем в автозагрузку:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/etc/init.d/ntpd start
</span><span class='line'>chkconfig ntpd on
</span><span class='line'>chkconfig --list ntpd</span></code></pre></td></tr></table></div></figure>


<p>Через некоторое время можно проверить статус синхронизации:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># ntpstat
</span><span class='line'>synchronised to NTP server (86.57.151.12) at stratum 5 
</span><span class='line'>   time correct to within 570 ms
</span><span class='line'>   polling server every 64 s</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Синхронизация файлов с помощью Lsyncd и Unison]]></title>
    <link href="http://Sibilia.github.com/blog/2013/03/26/sinkhronizatsiia-failov-s-pomoshchiu-lsyncd-i-unison/"/>
    <updated>2013-03-26T13:52:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/03/26/sinkhronizatsiia-failov-s-pomoshchiu-lsyncd-i-unison</id>
    <content type="html"><![CDATA[<p>Для быстрой синхронизации файлов между двумя серверами, когда изменения могут появиться на любом из них, прекрасно подходит связка из <a href="https://code.google.com/p/lsyncd/">lsyncd</a> и <a href="http://www.cis.upenn.edu/~bcpierce/unison/">unison</a>.</p>

<p>Lsyncd - это демон который слушает дерево каталогов и выполняет синхронизацию при событии (inotify или fsevents) на нём. Синхронизировать он может с помощью rsync или любым другим способом, который можно прописать у него в настройках в виде скрипта на Lua.</p>

<p>Unison позволяет синхронизировать файлы между серверами (или локально два различных каталога). В отличие от rsync он позволяет синхронизировать файлы одновременно в обе стороны. В качестве транспорта при синхронизации может быть использован ssh.
Пример буду приводить для CentOS/RedHat, для .deb систем отличия в мелочах. Начнём с установки:</p>

<!-- more -->


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install lsyncd unison</span></code></pre></td></tr></table></div></figure>


<p>Далее правим конфигурационный файл lsyncd: /etc/lsyncd.conf</p>

<figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>settings <span class="o">{</span>
</span><span class='line'>        <span class="nv">logfile</span>         <span class="o">=</span> <span class="s2">&quot;/mnt/lsynced.lsync.log&quot;</span>,
</span><span class='line'>        <span class="nv">statusFile</span>      <span class="o">=</span> <span class="s2">&quot;/mnt/lsynced.lsync.status&quot;</span>,
</span><span class='line'>        <span class="nv">statusInterval</span>  <span class="o">=</span> 10,
</span><span class='line'>        <span class="nv">maxDelays</span>       <span class="o">=</span> 3,
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="nv">runUnison</span> <span class="o">=</span> <span class="o">[[</span>/usr/bin/unison -retry 5 -owner -group -batch /mnt/lsynced ssh://node2//mnt/lsynced<span class="o">]]</span>
</span><span class='line'>
</span><span class='line'><span class="nv">runbash</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>        <span class="nv">onCreate</span> <span class="o">=</span> runUnison,
</span><span class='line'>        <span class="nv">onDelete</span> <span class="o">=</span> runUnison,
</span><span class='line'>        <span class="nv">onModify</span> <span class="o">=</span> runUnison,
</span><span class='line'>        <span class="nv">onMove</span> <span class="o">=</span> runUnison,
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'>sync <span class="o">{</span>
</span><span class='line'>        runbash,
</span><span class='line'>        <span class="nv">maxProcesses</span> <span class="o">=</span> 1,
</span><span class='line'>        <span class="nv">delay</span> <span class="o">=</span> 3,
</span><span class='line'>        <span class="nb">source</span> <span class="o">=</span> <span class="s2">&quot;/mnt/lsynced&quot;</span>,
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>На втором сервере меняем имя хоста в строке вызова unison. Проверить можно запустив lsyncd в режиме nodaemon (или без этой опции и читать логи):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>lsyncd -nodaemon /etc/lsyncd.conf</span></code></pre></td></tr></table></div></figure>


<p>Если всё отлично завершаем и переходим к настройке запуска lsyncd под corosync.
Тут есть два способа: использовать lsb ресурс corosync или написать свой ресурс агент. Я рассмотрю первый вариант. Для второго у меня на github лежит ресурс агент, но я его ещё не до конца протестировал.</p>

<h3>Corosync lsyncd LSB ресурс.</h3>

<p>Нам понадобится скрипт lsyncd daemon: /etc/init.d/lsyncd. Он должен входить в пакет lsyncd начиная с версии 2.1. Учитывая один момент в нём:</p>

<figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">LSYNCD_OPTIONS</span><span class="o">=</span><span class="s2">&quot;-pidfile /var/run/lsyncd.pid /etc/lsyncd.conf&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="k">if</span> <span class="o">[</span> -e /etc/sysconfig/lsyncd <span class="o">]</span>; <span class="k">then</span>
</span><span class='line'>  . /etc/sysconfig/lsyncd
</span><span class='line'><span class="k">fi</span>
</span></code></pre></td></tr></table></div></figure>


<p>нам обязательно надо прописать опции запуска в файле /etc/sysconfig/lsyncd:</p>

<figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">LSYNCD_OPTIONS</span><span class="o">=</span><span class="s2">&quot;-log scarce /etc/lsyncd.conf&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Опция &#8220;-log scarce&#8221; уменьшает уровень логирования.
Теперь осталось добавить lsyncd в конфигурацию corosync:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>crm configure primitive lsyncd-node1 lsb:lsyncd op monitor interval="5" meta target-role="Started"
</span><span class='line'>crm configure location loc-lsyncd-on-node1 lsyncd-node1 inf: node1</span></code></pre></td></tr></table></div></figure>


<p>На втором сервере делаем соответствующие поправки.
Вот в принципе и всё. Данное решение синхронизации хорошо себя показало на высоких нагрузках. Если есть вопросы, я постораюсь помочь.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Установка crm shell в Pacemaker]]></title>
    <link href="http://Sibilia.github.com/blog/2013/03/14/ustanovka-crm-shell-v-pacemaker/"/>
    <updated>2013-03-14T14:36:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/03/14/ustanovka-crm-shell-v-pacemaker</id>
    <content type="html"><![CDATA[<p>В начале марта в проекте Pacemaker произошли некоторые изменения, в частности был вырезан crm shell начиная с версии pacemaker-1.1.7.</p>

<blockquote><p>Since late-April, the crm shell is no longer included in the Pacemaker source tree. This change was made at the author&#8217;s request as it is now maintained as a separate project.</p><footer><strong>ClusterLabs</strong> <cite><a href='https://github.com/ClusterLabs/pacemaker#important-information-about-the-crm-shell'>Important Information About the Crm Shell</a></cite></footer></blockquote>


<!-- more -->


<p>Это довольно печально, так как crm shell очень удобный инструмент для ручного управления кластером. Для его добавления необходимо доустановить пакеты &#8220;crmsh&#8221; и &#8220;pssh&#8221;. Скачать их можно с репозитория <a href="http://download.opensuse.org/repositories/network:/ha-clustering/">crm shell</a>.
Для CentOS 6 x86-64 достаточно следующее:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install pacemaker corosync
</span><span class='line'>cd /tmp
</span><span class='line'>wget http://download.opensuse.org/repositories/network:/ha-clustering/CentOS_CentOS-6/x86_64/crmsh-1.2.5-55.2.x86_64.rpm
</span><span class='line'>wget http://download.opensuse.org/repositories/network:/ha-clustering/CentOS_CentOS-6/x86_64/pssh-2.3.1-15.1.x86_64.rpm
</span><span class='line'>yum install ./crmsh*.rpm ./pssh*.rpm</span></code></pre></td></tr></table></div></figure>


<p>На данный момент пакетов crmsh и pssh отсутствуют в официальных репозитариях RedHat/CentOS и в EPEL. Быть может в скором времени их туда добавят, или clusterlabs уладят совместимость.</p>

<h3>P.S.</h3>

<p>Вот кстати мой скрипт для отчистки failcount в выводе crm_mon -fnr. Это порой необходимо для востановления кластера после сбоя и обнуления счётчиков.</p>

<figure class='code'><figcaption><span> (corosync_clear)</span> <a href='http://Sibilia.github.com/downloads/code/corosync_clear'>download</a></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c">#!/bin/bash</span>
</span><span class='line'><span class="c"># Если значение failcount=1000000 у ресурса, то сбрасывается его состояние (crm resource cleanup &lt;res&gt;).</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Author: Ilia Sibiryatkin &lt;Sibvilian@gmail.com&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nv">NODE</span><span class="o">=</span><span class="sb">`</span>crm_node -l |cut -d<span class="s1">&#39; &#39;</span> -f 2<span class="sb">`</span>
</span><span class='line'>
</span><span class='line'><span class="k">for </span>STR in <span class="k">$(</span>crm_mon -f1 | grep fail-count| awk <span class="s1">&#39;{ print $1 &quot;@&quot; $3 }&#39;</span><span class="k">)</span> ; <span class="k">do</span>
</span><span class='line'><span class="k">  </span><span class="nv">RES</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span> <span class="nv">$STR</span> |awk <span class="s1">&#39;BEGIN {FS=&quot;@&quot;}{print (substr($1,0,length($1)-1))}&#39;</span><span class="k">)</span>
</span><span class='line'>        <span class="nv">FAIL</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span> <span class="nv">$STR</span> |awk <span class="s1">&#39;BEGIN {FS=&quot;@&quot;}{print (substr($2,12,length($2)))}&#39;</span><span class="k">)</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">if</span> <span class="o">[[</span> <span class="nv">$FAIL</span> <span class="o">==</span> 1000000 <span class="o">]]</span>; <span class="k">then</span>
</span><span class='line'><span class="k">                </span><span class="nb">echo</span> <span class="s2">&quot;cleanup $RES&quot;</span>
</span><span class='line'>                crm resource cleanup <span class="nv">$RES</span>
</span><span class='line'>        <span class="k">else</span>
</span><span class='line'><span class="k">                </span><span class="nb">echo</span> <span class="s2">&quot;clear fail-count $RES&quot;</span>
</span><span class='line'>                <span class="k">for </span>i in <span class="nv">$NODE</span> ; <span class="k">do</span>
</span><span class='line'><span class="k">                        </span>crm resource failcount <span class="nv">$RES</span> delete <span class="nv">$i</span>
</span><span class='line'>                <span class="k">done </span>
</span><span class='line'><span class="k">        fi</span>
</span><span class='line'><span class="k">done</span>
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Перенаправление http]]></title>
    <link href="http://Sibilia.github.com/blog/2013/03/04/pierienapravlieniie-http/"/>
    <updated>2013-03-04T13:53:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/03/04/pierienapravlieniie-http</id>
    <content type="html"><![CDATA[<p>Возникла необходимость по наступлению определённого события временно перенаправлять http соединения на другой порт, где слушает nginx с заглушкой. Это легко можно сделать с помошью парочки правил в iptables. Приведу несколько основных моментов из bash скрипта.
Нам нужны две таблицы в новой цепочке: &#8220;nat&#8221; для непосредственно перенаправления и &#8220;filter&#8221; для сброса активных сессий. Фильтровать активные сессии необходимо потому, что в таблицу &#8220;nat&#8221;, цепочки PREROUTING, поподают лишь tcp соединения с состоянием NEW. Активные tcp соединения живут примерно 2-3 минуты. Если это не критично, то можно обойтись без их фильтрации. Для этого просто не добавляем все правила в которых указана таблица &#8220;filter&#8221;.
Для начала создадим новую цепочку. Это удобно тем что её можно очистить полностью, когда необходимо вернуть всё к изначальному состоянию.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>iptables -t nat -N HTTP_REDIR
</span><span class='line'>iptables -t filter -N HTTP_REDIR</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<p>Теперь создадим два правила в цепочках PREROUTING и INPUT в которых будем направлять tcp в нашу цепочку. Эти правила не будут удаляться.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>iptables -t nat -I PREROUTING -p tcp -j HTTP_REDIR
</span><span class='line'>iptables -t filter -I INPUT -p tcp -j HTTP_REDIR</span></code></pre></td></tr></table></div></figure>


<p>Теперь, если нам необходимо включить перенаправление, то добавляем правила в нашу цепочку:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>iptables -t nat -A HTTP_REDIR -p tcp --dport 80 -j REDIRECT --to-port 20302
</span><span class='line'>iptables -t filter -A HTTP_REDIR -p tcp --dport 80 -j REJECT --reject-with tcp-reset</span></code></pre></td></tr></table></div></figure>


<p>Если необходимо перенаправлять к примеру ещё и https то можно вместо &#8220;&#8211;dport 80&#8221; указать &#8220;-m multiport &#8211;dports 80,443&#8221;
Чтобы убрать перенаправление, просто чистим нашу цепочку.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>iptables -t nat -F HTTP_REDIR
</span><span class='line'>iptables -t filter -F HTTP_REDIR</span></code></pre></td></tr></table></div></figure>


<p>Вот в принципе и всё, за всеми подробностями идём в маны iptables, написано там всё очень подробно.</p>

<h4>P.S.</h4>

<p>Нашел более удобный способ сброса активный tcp соединений - conntrack.
Пакет называется по разному, в Debian вроде просто conntrack, в CentOS/RedHat conntrack-tool.
После установки, у нас появляется возможность удобно посмотреть активные соединения и их статус:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>conntrack -L -p tcp --dport 80</span></code></pre></td></tr></table></div></figure>


<p>Так же удобно их разом очистить:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>conntrack -D -p tcp --dport 80</span></code></pre></td></tr></table></div></figure>


<p>Фильтровать он по multiport не умеет. Суть заключается в том, что просто чистится в ядре таблица соединений tcp по определённому порту. Входящий пакет после этого со статусом не NEW не находится в таблице и сбрасывается с tcp-reset.
В таком случае нет нужды использовать таблицу filter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Установка и настройка Munin]]></title>
    <link href="http://Sibilia.github.com/blog/2013/02/18/ustanovka-i-nastroika-munin/"/>
    <updated>2013-02-18T15:07:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/02/18/ustanovka-i-nastroika-munin</id>
    <content type="html"><![CDATA[<p><a href="http://munin-monitoring.org/">Munin</a> - Это мощная клиент-серверная система мониторинга параметров серверов. Главный сервер munin запускается по cron и опрашивает munin-node сервера собирая с них данные и рисует красивые и наглядные графики. На munin-node серверах демон при подключении главного сервера запускает скрипты плагинов из /etc/munin/plugins/. Плагинов в стандартной установке большое количество. Можно написать и свои.</p>

<!-- more -->


<h3>Установка</h3>

<p>Приведу пример установки клиента и сервера для CentOS/RHEL. Для Debian/Ubuntu всё почти аналогично.
Для начала установим главный сервер:</p>

<pre><code># yum install munin munin-node
</code></pre>

<p>Для клиентов необходим только munin-node:</p>

<pre><code># yum install munin-node
</code></pre>

<h3>Настройка</h3>

<p>Главный файл настройки сервера Munin /etc/munin/munin.conf. В нём необходимо прописать клиентов:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># a simple host tree
</span><span class='line'>[node1]
</span><span class='line'>  address 172.16.0.12
</span><span class='line'>  use_node_name yes
</span><span class='line'>
</span><span class='line'>[node2]
</span><span class='line'>  address 172.16.0.13
</span><span class='line'>    use_node_name yes
</span><span class='line'>
</span><span class='line'>[bckp]
</span><span class='line'>    address 127.0.0.1
</span><span class='line'>    use_node_name yes</span></code></pre></td></tr></table></div></figure>


<p>Для настройки клиентов нам необходимо подправить /etc/munin/munin-node.conf:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Разрешаем подключаться серверу:
</span><span class='line'>allow ^127\.0\.0\.1$
</span><span class='line'>allow ^172\.16\.0\.10$ # ip address bckp 
</span><span class='line'>
</span><span class='line'># указываем host name и ip
</span><span class='line'>host_name node1
</span><span class='line'>host 172.16.0.12</span></code></pre></td></tr></table></div></figure>


<p>Теперь включим парочку дополнительных плагинов:</p>

<pre><code>ln -s /usr/share/munin/plugins/acpi /etc/munin/plugins/
ln -s /usr/share/munin/plugins/iostat /etc/munin/plugins/
ln -s /usr/share/munin/plugins/iostat_ios /etc/munin/plugins/
</code></pre>

<p>Как видно из примера плагины подключаются созданием симлинка в /etc/munin/plugins/</p>

<h5>Подключение плагинов для MongoDB</h5>

<p>Плагины для MongoDB не идут в стандартной поставке, но их не сложно установить.</p>

<pre><code># wget https://github.com/erh/mongo-munin/archive/master.zip -O /tmp/mongo-munin.zip
# unzip /tmp/mongo-munin.zip /tmp/
# mkdir -p /usr/local/share/munin/plugins
# cp /tmp/mongo-munin-master/mongo_* /usr/local/share/munin/plugins/

# ln -s /usr/local/share/munin/plugins/mongo_btree /etc/munin/plugins/
# ln -s /usr/local/share/munin/plugins/mongo_conn /etc/munin/plugins/
# ln -s /usr/local/share/munin/plugins/mongo_lock /etc/munin/plugins/
# ln -s /usr/local/share/munin/plugins/mongo_mem /etc/munin/plugins/
# ln -s /usr/local/share/munin/plugins/mongo_ops /etc/munin/plugins/
</code></pre>

<p>Не забываем перезагружать munin после правки конфигов и добавления/удаления плагинов.</p>

<pre><code># /etc/init.d/munin-node restart
</code></pre>

<p>Можно проверить просто запустив один из скриптов. Первоначально у меня они не отрабатывали:</p>

<pre><code># /usr/local/share/munin/plugins/mongo_btree
...
  File "/usr/lib64/python2.6/json/decoder.py", line 319, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib64/python2.6/json/decoder.py", line 338, in raw_decode
    raise ValueError("No JSON object could be decoded")
ValueError: No JSON object could be decoded
</code></pre>

<p>Подравив в скрипте вывод значения я выяснил в чём проблема&#8230;</p>

<pre><code># vim /usr/local/share/munin/plugins/mongo_btree
</code></pre>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>28     raw = urllib2.urlopen(req).read()
</span><span class='line'>29     print raw
</span><span class='line'>30     return json.loads( raw )["serverStatus"]</span></code></pre></td></tr></table></div></figure>


<pre><code># /usr/local/share/munin/plugins/mongo_btree
You are trying to access MongoDB on the native driver port. For http diagnostic access, add 1000 to the port number
....
</code></pre>

<p>Сново правим скрипт не забыв удалить добавленную строчку на прошлом этапе.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>url = "http://%s:%d/_status" % (host, port+1000)</span></code></pre></td></tr></table></div></figure>


<p>Теперь всё в порядке.</p>

<pre><code># /usr/local/share/munin/plugins/mongo_btree
missRatio.value 0
resets.value 0
hits.value 23325605
misses.value 46
accesses.value 23325651
</code></pre>

<p>Не забываем сделать соответствующие поправки в остальных скриптах mongo_</p>

<h5>Подключение плагинов для MySQL</h5>

<p>В стандартной поставке есть плагины для mysql, если нужно, подключаем и их:</p>

<pre><code># ln -s /usr/share/munin/plugins/mysql_* /etc/munin/plugins
</code></pre>

<p>Для их работы необходимы дополнительные пакеты для perl:</p>

<pre><code># yum install perl-Cache-Cache perl-DBD-MySQL perl-IPC-ShareLite
</code></pre>

<p>Если ещё чего не хватит, то это легко выяснить чтением логов /var/log/munin-node/munin-node.log.
Создаём в mysql пользователя для munin:</p>

<pre><code># mysql -u root -p -e 'create user munin'
</code></pre>

<p>Подправим файл /etc/munin/plugin-conf.d/munin-node:</p>

<pre><code>[mysql*]
    user root
    env.mysqlopts --defaults-file=/etc/mysql/my.cnf
    env.mysqluser munin
</code></pre>

<h5>Добавление в автозагрузку</h5>

<p>Добавить в автозагрузку не сложно. Необходимо выполнить на каждом сервере:</p>

<pre><code># chkconfig --add munin-node
# chkconfig munin-node on
</code></pre>

<p>Проверяем:</p>

<pre><code># chkconfig --list munin-node
munin-node      0:off   1:off   2:on    3:on    4:on    5:on    6:off
</code></pre>

<h5>Использование.</h5>

<p>На главном сервере Munin необходим Web сервер. В файле /etc/munin/munin.conf прописывается куда будут генерироваться итоговые .html
Например для стандартных настроек достаточно просто в браузере перейти на ip адресс главного сервера munin: http://172.16.0.10/munin</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Решение некоторых проблем в DRBD]]></title>
    <link href="http://Sibilia.github.com/blog/2013/01/29/rieshieniie-niekotorykh-probliem-v-drbd/"/>
    <updated>2013-01-29T14:39:00+03:00</updated>
    <id>http://Sibilia.github.com/blog/2013/01/29/rieshieniie-niekotorykh-probliem-v-drbd</id>
    <content type="html"><![CDATA[<p>Рассмотрю кратко решение проблем в DRBD Diskless и Split-brain.</p>

<h3>DRBD Diskless</h3>

<p>При выходе из строя дискового массива и его восстановления  можно получить следующую ситуацию в drbd:</p>

<pre><code># drbd-overview
2:r2 Connected Secondary/Primary Diskless/UpToDate C r----
</code></pre>

<h4>Для решения этой проблемы необходимо сбросить мета данные:</h4>

<p>На активной ноде необходимо отмонтировать раздел. Затем на неактивной ноде отключаем ресурс:</p>

<pre><code># drbdadm down r2
</code></pre>

<p>Создаем заново блок мета-данных:</p>

<pre><code># drbdadm create-md r2
 ...
New drbd meta data block successfully created.
</code></pre>

<!-- more -->


<p>Включаем ресурс обратно (должна начаться синхронизация):</p>

<pre><code># drbdadm up r2
# drbdadm connect r2
# drbd-overview 
2:r2 SyncTarget Secondary/Primary Inconsistent/UpToDate C r---- 
[&gt;....................] sync'ed: 0.1% (31796/31796)M queue_delay: 0.0 ms 
</code></pre>

<p>Для изменения скорости синхронизации можно ввести:</p>

<pre><code># drbdsetup /dev/drbd2 syncer -r 10M
</code></pre>

<p>Для задания в настройках необходимо прописать в /etc/drbd.conf :</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>syncer {
</span><span class='line'>  rate 100M;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h3>DRBD Split-brain</h3>

<p>Ещё бывает ситуация когда ноды не синхронизируются со следующими признаками:</p>

<pre><code># drbd-overview
3:just StandAlone Primary/Unknown UpToDate/DUnknown r----- /mnt/Just ext3 5.0G 156M 4.6G 4%
</code></pre>

<p>Причиной этого может стать состояние split-brain. Для решения этой проблемы необходимо:
На secondary:</p>

<pre><code># drbdadm disconnect just
# drbdadm -- --discard-my-data connect just
</code></pre>

<p>На primary:</p>

<pre><code># drbdadm connect just
</code></pre>

<p>После синхронизации (если она необходима) всё должно работать:</p>

<pre><code># drbd-overview
3:just Connected Primary/Secondary UpToDate/UpToDate C r----- /mnt/Just ext3 5.0G 156M 4.6G 4%
</code></pre>
]]></content>
  </entry>
  
</feed>
